/* CUDA NeoScrypt kernel by John Doering <ghostlander@phoenixcoin.org>
 * Influenced by the earlier work of djm34, tpruvot, pallas1, alexis78, etc.
 * Distributed under the terms of the GNU General Public Licence, version 3 */

#include <stdio.h>
#include <memory.h>

#include <cuda.h>
#include <cuda_runtime.h>

#ifndef MAX_GPUS
#define MAX_GPUS 32
#endif

#ifdef _MSC_VER
typedef unsigned int uint;
typedef unsigned long long ulong;
#endif

#if defined(_WIN64) || defined(__LP64__)
#define __MEM_PTR "l"
#else
#define __MEM_PTR "r"
#endif

typedef struct __align__(16) uint8 {
    uint s0, s1, s2, s3, s4, s5, s6, s7;
} uint8;

typedef struct __align__(16) ulong8 {
    ulong s0, s1, s2, s3, s4, s5, s6, s7;
} ulong8;

typedef struct __align__(16) uint16 {
    union {
        struct { uint  a0, a1, a2, a3, a4, a5, a6, a7; };
        uint8 a;
    };
    union {
        struct { uint  b0, b1, b2, b3, b4, b5, b6, b7; };
        uint8 b;
    };
} uint16;

typedef struct __align__(16) uint64 {
    union {
        struct { uint  a0, a1, a2, a3, a4, a5, a6, a7; };
        uint8 a;
    };
    union {
        struct { uint  b0, b1, b2, b3, b4, b5, b6, b7; };
        uint8 b;
    };
    union {
        struct { uint  c0, c1, c2, c3, c4, c5, c6, c7; };
        uint8 c;
    };
    union {
        struct { uint  d0, d1, d2, d3, d4, d5, d6, d7; };
        uint8 d;
    };
    union {
        struct { uint  e0, e1, e2, e3, e4, e5, e6, e7; };
        uint8 e;
    };
    union {
        struct { uint  f0, f1, f2, f3, f4, f5, f6, f7; };
        uint8 f;
    };
    union {
        struct { uint  g0, g1, g2, g3, g4, g5, g6, g7; };
        uint8 g;
    };
    union {
        struct { uint  h0, h1, h2, h3, h4, h5, h6, h7; };
        uint8 h;
    };
} uint64;

static __forceinline__ __host__ __device__ void operator^=(uint4 &a, uint4 &b) {
      a.x ^= b.x;
      a.y ^= b.y;
      a.z ^= b.z;
      a.w ^= b.w;
}

static __forceinline__ __host__ __device__ void operator+=(uint4 &a, uint4 &b) {
      a.x += b.x;
      a.y += b.y;
      a.z += b.z;
      a.w += b.w;
}

static __forceinline__ __host__ __device__ void operator^=(uint8 &a, uint8 &b) {
      a.s0 ^= b.s0;
      a.s1 ^= b.s1;
      a.s2 ^= b.s2;
      a.s3 ^= b.s3;
      a.s4 ^= b.s4;
      a.s5 ^= b.s5;
      a.s6 ^= b.s6;
      a.s7 ^= b.s7;
}

__device__ uint8 *G;
__device__ uint8 *Tr;
__device__ uint8 *Tr2;
__device__ uint8 *Input;

static uint *Nonce[MAX_GPUS];

__constant__ uint hash_target;
__constant__ __align__(16) uint key_init[16]; 
__constant__ __align__(16) uint input_init[16];
__constant__ __align__(16) uint c_data[64];

static const uint8 BLAKE2s_IV_host = {
    0x6A09E667, 0xBB67AE85, 0x3C6EF372, 0xA54FF53A,
    0x510E527F, 0x9B05688C, 0x1F83D9AB, 0x5BE0CD19
};

static const uint BLAKE2S_SIGMA_host[10][16] = {
    { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15 },
    { 14, 10, 4, 8, 9, 15, 13, 6, 1, 12, 0, 2, 11, 7, 5, 3 },
    { 11, 8, 12, 0, 5, 2, 15, 13, 10, 14, 3, 6, 7, 1, 9, 4 },
    { 7, 9, 3, 1, 13, 12, 11, 14, 2, 6, 5, 10, 4, 0, 15, 8 },
    { 9, 0, 5, 7, 2, 4, 10, 15, 14, 1, 11, 12, 6, 8, 3, 13 },
    { 2, 12, 6, 10, 0, 11, 8, 3, 4, 13, 7, 5, 15, 14, 1, 9 },
    { 12, 5, 1, 15, 14, 13, 4, 10, 0, 7, 6, 3, 9, 2, 8, 11 },
    { 13, 11, 7, 14, 12, 1, 3, 9, 5, 0, 15, 4, 8, 6, 2, 10 },
    { 6, 15, 14, 9, 11, 3, 0, 8, 12, 2, 13, 7, 1, 4, 10, 5 },
    { 10, 2, 8, 4, 7, 6, 1, 5, 15, 11, 9, 14, 3, 12, 13, 0 },
};

static const __constant__ uint8 BLAKE2s_IV = {
    0x6A09E667, 0xBB67AE85, 0x3C6EF372, 0xA54FF53A,
    0x510E527F, 0x9B05688C, 0x1F83D9AB, 0x5BE0CD19
};

#ifndef ROTR32
#define ROTR32(a,b) (((a) >> (b)) | ((a) << (32 - b)))
#endif

#define Ghost(idx0, idx1, a, b, c, d, key) { \
    idx = BLAKE2S_SIGMA_host[idx0][idx1]; \
    a += key[idx]; \
    a += b; d = ROTR32(d ^ a,16); \
    c += d; b = ROTR32(b ^ c, 12); \
    idx = BLAKE2S_SIGMA_host[idx0][idx1+1]; \
    a += key[idx]; \
    a += b; d = ROTR32(d ^ a,8); \
    c += d; b = ROTR32(b ^ c, 7); \
}


static __forceinline__ __device__
void blake2s(uint *out, const uint * __restrict__ inout,
  const  uint * __restrict__ key) {
    const uint c16 = 0x1032, c20 = 20, c24 = 0x0321, c25 = 25, c0 = 0;
    uint8 V0, V1, T;

    V1 = BLAKE2s_IV;
    V0 = BLAKE2s_IV;
    V0.s0 ^= 0x01012020;

    T = V0;

    V1.s4 ^= 64;


//    G(V0.s0, V0.s4, V1.s0, V1.s4, key[0], key[1]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s4), "+r"(V1.s0), "+r"(V1.s4)
        : "r"(key[0]), "r"(key[1]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s5, V1.s1, V1.s5, key[2], key[3]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s5), "+r"(V1.s1), "+r"(V1.s5)
        : "r"(key[2]), "r"(key[3]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s6, V1.s2, V1.s6, key[4], key[5]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s6), "+r"(V1.s2), "+r"(V1.s6)
        : "r"(key[4]), "r"(key[5]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s3, V0.s7, V1.s3, V1.s7, key[6], key[7]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s7), "+r"(V1.s3), "+r"(V1.s7)
        : "r"(key[6]), "r"(key[7]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE0(V0.s0, V0.s5, V1.s2, V1.s7);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %4;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s5), "+r"(V1.s2), "+r"(V1.s7)
        : "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE0(V0.s1, V0.s6, V1.s3, V1.s4);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %4;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s6), "+r"(V1.s3), "+r"(V1.s4)
        : "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE0(V0.s2, V0.s7, V1.s0, V1.s5);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %4;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s7), "+r"(V1.s0), "+r"(V1.s5)
        : "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE0(V0.s3, V0.s4, V1.s1, V1.s6);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %4;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s4), "+r"(V1.s1), "+r"(V1.s6)
        : "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));


//    G_PRE0(V0.s0, V0.s4, V1.s0, V1.s4);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %4;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s4), "+r"(V1.s0), "+r"(V1.s4)
        : "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE1(V0.s1, V0.s5, V1.s1, V1.s5, key[4]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s5), "+r"(V1.s1), "+r"(V1.s5)
        : "r"(key[4]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE0(V0.s2, V0.s6, V1.s2, V1.s6);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %4;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s6), "+r"(V1.s2), "+r"(V1.s6)
        : "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE2(V0.s3, V0.s7, V1.s3, V1.s7, key[6]);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s7), "+r"(V1.s3), "+r"(V1.s7)
        : "r"(key[6]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE1(V0.s0, V0.s5, V1.s2, V1.s7, key[1]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s5), "+r"(V1.s2), "+r"(V1.s7)
        : "r"(key[1]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s6, V1.s3, V1.s4, key[0], key[2]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s6), "+r"(V1.s3), "+r"(V1.s4)
        : "r"(key[0]), "r"(key[2]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE2(V0.s2, V0.s7, V1.s0, V1.s5, key[7]);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s7), "+r"(V1.s0), "+r"(V1.s5)
        : "r"(key[7]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s3, V0.s4, V1.s1, V1.s6, key[5], key[3]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s4), "+r"(V1.s1), "+r"(V1.s6)
        : "r"(key[5]), "r"(key[3]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));


//    G_PRE0(V0.s0, V0.s4, V1.s0, V1.s4);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %4;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s4), "+r"(V1.s0), "+r"(V1.s4)
        : "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE2(V0.s1, V0.s5, V1.s1, V1.s5, key[0]);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s5), "+r"(V1.s1), "+r"(V1.s5)
        : "r"(key[0]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s6, V1.s2, V1.s6, key[5], key[2]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s6), "+r"(V1.s2), "+r"(V1.s6)
        : "r"(key[5]), "r"(key[2]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE0(V0.s3, V0.s7, V1.s3, V1.s7);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %4;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s7), "+r"(V1.s3), "+r"(V1.s7)
        : "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE0(V0.s0, V0.s5, V1.s2, V1.s7);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %4;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s5), "+r"(V1.s2), "+r"(V1.s7)
        : "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s6, V1.s3, V1.s4, key[3], key[6]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s6), "+r"(V1.s3), "+r"(V1.s4)
        : "r"(key[3]), "r"(key[6]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s7, V1.s0, V1.s5, key[7], key[1]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s7), "+r"(V1.s0), "+r"(V1.s5)
        : "r"(key[7]), "r"(key[1]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE2(V0.s3, V0.s4, V1.s1, V1.s6, key[4]);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s4), "+r"(V1.s1), "+r"(V1.s6)
        : "r"(key[4]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));


//    G_PRE1(V0.s0, V0.s4, V1.s0, V1.s4, key[7]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s4), "+r"(V1.s0), "+r"(V1.s4)
        : "r"(key[7]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s5, V1.s1, V1.s5, key[3], key[1]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s5), "+r"(V1.s1), "+r"(V1.s5)
        : "r"(key[3]), "r"(key[1]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE0(V0.s2, V0.s6, V1.s2, V1.s6);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %4;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s6), "+r"(V1.s2), "+r"(V1.s6)
        : "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE0(V0.s3, V0.s7, V1.s3, V1.s7);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %4;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s7), "+r"(V1.s3), "+r"(V1.s7)
        : "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s0, V0.s5, V1.s2, V1.s7, key[2], key[6]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s5), "+r"(V1.s2), "+r"(V1.s7)
        : "r"(key[2]), "r"(key[6]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE1(V0.s1, V0.s6, V1.s3, V1.s4, key[5]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s6), "+r"(V1.s3), "+r"(V1.s4)
        : "r"(key[5]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s7, V1.s0, V1.s5, key[4], key[0]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s7), "+r"(V1.s0), "+r"(V1.s5)
        : "r"(key[4]), "r"(key[0]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE0(V0.s3, V0.s4, V1.s1, V1.s6);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %4;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s4), "+r"(V1.s1), "+r"(V1.s6)
        : "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));


//    G_PRE2(V0.s0, V0.s4, V1.s0, V1.s4, key[0]);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s4), "+r"(V1.s0), "+r"(V1.s4)
        : "r"(key[0]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s5, V1.s1, V1.s5, key[5], key[7]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s5), "+r"(V1.s1), "+r"(V1.s5)
        : "r"(key[5]), "r"(key[7]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s6, V1.s2, V1.s6, key[2], key[4]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s6), "+r"(V1.s2), "+r"(V1.s6)
        : "r"(key[2]), "r"(key[4]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE0(V0.s3, V0.s7, V1.s3, V1.s7);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %4;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s7), "+r"(V1.s3), "+r"(V1.s7)
        : "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE2(V0.s0, V0.s5, V1.s2, V1.s7, key[1]);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s5), "+r"(V1.s2), "+r"(V1.s7)
        : "r"(key[1]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE0(V0.s1, V0.s6, V1.s3, V1.s4);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %4;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s6), "+r"(V1.s3), "+r"(V1.s4)
        : "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE1(V0.s2, V0.s7, V1.s0, V1.s5, key[6]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s7), "+r"(V1.s0), "+r"(V1.s5)
        : "r"(key[6]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE1(V0.s3, V0.s4, V1.s1, V1.s6, key[3]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s4), "+r"(V1.s1), "+r"(V1.s6)
        : "r"(key[3]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));


//    G_PRE1(V0.s0, V0.s4, V1.s0, V1.s4, key[2]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s4), "+r"(V1.s0), "+r"(V1.s4)
        : "r"(key[2]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE1(V0.s1, V0.s5, V1.s1, V1.s5, key[6]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s5), "+r"(V1.s1), "+r"(V1.s5)
        : "r"(key[6]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE1(V0.s2, V0.s6, V1.s2, V1.s6, key[0]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s6), "+r"(V1.s2), "+r"(V1.s6)
        : "r"(key[0]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE2(V0.s3, V0.s7, V1.s3, V1.s7, key[3]);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s7), "+r"(V1.s3), "+r"(V1.s7)
        : "r"(key[3]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE1(V0.s0, V0.s5, V1.s2, V1.s7, key[4]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s5), "+r"(V1.s2), "+r"(V1.s7)
        : "r"(key[4]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s6, V1.s3, V1.s4, key[7], key[5]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s6), "+r"(V1.s3), "+r"(V1.s4)
        : "r"(key[7]), "r"(key[5]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE0(V0.s2, V0.s7, V1.s0, V1.s5);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %4;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s7), "+r"(V1.s0), "+r"(V1.s5)
        : "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE1(V0.s3, V0.s4, V1.s1, V1.s6, key[1]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s4), "+r"(V1.s1), "+r"(V1.s6)
        : "r"(key[1]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));


//    G_PRE2(V0.s0, V0.s4, V1.s0, V1.s4, key[5]);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s4), "+r"(V1.s0), "+r"(V1.s4)
        : "r"(key[5]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE1(V0.s1, V0.s5, V1.s1, V1.s5, key[1]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s5), "+r"(V1.s1), "+r"(V1.s5)
        : "r"(key[1]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE0(V0.s2, V0.s6, V1.s2, V1.s6);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %4;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s6), "+r"(V1.s2), "+r"(V1.s6)
        : "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE1(V0.s3, V0.s7, V1.s3, V1.s7, key[4]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s7), "+r"(V1.s3), "+r"(V1.s7)
        : "r"(key[4]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s0, V0.s5, V1.s2, V1.s7, key[0], key[7]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s5), "+r"(V1.s2), "+r"(V1.s7)
        : "r"(key[0]), "r"(key[7]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s6, V1.s3, V1.s4, key[6], key[3]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s6), "+r"(V1.s3), "+r"(V1.s4)
        : "r"(key[6]), "r"(key[3]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE2(V0.s2, V0.s7, V1.s0, V1.s5, key[2]);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s7), "+r"(V1.s0), "+r"(V1.s5)
        : "r"(key[2]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE0(V0.s3, V0.s4, V1.s1, V1.s6);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %4;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s4), "+r"(V1.s1), "+r"(V1.s6)
        : "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));


//    G_PRE0(V0.s0, V0.s4, V1.s0, V1.s4);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %4;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s4), "+r"(V1.s0), "+r"(V1.s4)
        : "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE1(V0.s1, V0.s5, V1.s1, V1.s5, key[7]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s5), "+r"(V1.s1), "+r"(V1.s5)
        : "r"(key[7]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE2(V0.s2, V0.s6, V1.s2, V1.s6, key[1]);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s6), "+r"(V1.s2), "+r"(V1.s6)
        : "r"(key[1]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE1(V0.s3, V0.s7, V1.s3, V1.s7, key[3]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s7), "+r"(V1.s3), "+r"(V1.s7)
        : "r"(key[3]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s0, V0.s5, V1.s2, V1.s7, key[5], key[0]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s5), "+r"(V1.s2), "+r"(V1.s7)
        : "r"(key[5]), "r"(key[0]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE2(V0.s1, V0.s6, V1.s3, V1.s4, key[4]);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s6), "+r"(V1.s3), "+r"(V1.s4)
        : "r"(key[4]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE2(V0.s2, V0.s7, V1.s0, V1.s5, key[6]);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s7), "+r"(V1.s0), "+r"(V1.s5)
        : "r"(key[6]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s3, V0.s4, V1.s1, V1.s6, key[2], key[10]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s4), "+r"(V1.s1), "+r"(V1.s6)
        : "r"(key[2]), "r"(key[10]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));


//    G_PRE1(V0.s0, V0.s4, V1.s0, V1.s4, key[6]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s4), "+r"(V1.s0), "+r"(V1.s4)
        : "r"(key[6]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE0(V0.s1, V0.s5, V1.s1, V1.s5);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %4;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s5), "+r"(V1.s1), "+r"(V1.s5)
        : "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE2(V0.s2, V0.s6, V1.s2, V1.s6, key[3]);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s6), "+r"(V1.s2), "+r"(V1.s6)
        : "r"(key[3]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE1(V0.s3, V0.s7, V1.s3, V1.s7, key[0]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s7), "+r"(V1.s3), "+r"(V1.s7)
        : "r"(key[0]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE2(V0.s0, V0.s5, V1.s2, V1.s7, key[2]);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s5), "+r"(V1.s2), "+r"(V1.s7)
        : "r"(key[2]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE2(V0.s1, V0.s6, V1.s3, V1.s4, key[7]);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s6), "+r"(V1.s3), "+r"(V1.s4)
        : "r"(key[7]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s7, V1.s0, V1.s5, key[1], key[4]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s7), "+r"(V1.s0), "+r"(V1.s5)
        : "r"(key[1]), "r"(key[4]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE2(V0.s3, V0.s4, V1.s1, V1.s6, key[5]);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s4), "+r"(V1.s1), "+r"(V1.s6)
        : "r"(key[5]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));


//    G_PRE2(V0.s0, V0.s4, V1.s0, V1.s4, key[2]);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s4), "+r"(V1.s0), "+r"(V1.s4)
        : "r"(key[2]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE2(V0.s1, V0.s5, V1.s1, V1.s5, key[4]);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s5), "+r"(V1.s1), "+r"(V1.s5)
        : "r"(key[4]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s6, V1.s2, V1.s6, key[7], key[6]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s6), "+r"(V1.s2), "+r"(V1.s6)
        : "r"(key[7]), "r"(key[6]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s3, V0.s7, V1.s3, V1.s7, key[1], key[5]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s7), "+r"(V1.s3), "+r"(V1.s7)
        : "r"(key[1]), "r"(key[5]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE0(V0.s0, V0.s5, V1.s2, V1.s7);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %4;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s5), "+r"(V1.s2), "+r"(V1.s7)
        : "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE0(V0.s1, V0.s6, V1.s3, V1.s4);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %4;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s6), "+r"(V1.s3), "+r"(V1.s4)
        : "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE1(V0.s2, V0.s7, V1.s0, V1.s5, key[3]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s7), "+r"(V1.s0), "+r"(V1.s5)
        : "r"(key[3]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G_PRE2(V0.s3, V0.s4, V1.s1, V1.s6, key[0]);
    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %9, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %8;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s4), "+r"(V1.s1), "+r"(V1.s6)
        : "r"(key[0]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));


// V0 ^= V1
    asm("{\n"
      "xor.b32 %0, %0, %8;\n"
      "xor.b32 %1, %1, %9;\n"
      "xor.b32 %2, %2, %10;\n"
      "xor.b32 %3, %3, %11;\n"
      "xor.b32 %4, %4, %12;\n"
      "xor.b32 %5, %5, %13;\n"
      "xor.b32 %6, %6, %14;\n"
      "xor.b32 %7, %7, %15;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s1), "+r"(V0.s2), "+r"(V0.s3),
          "+r"(V0.s4), "+r"(V0.s5), "+r"(V0.s6), "+r"(V0.s7)
        : "r"(V1.s0), "r"(V1.s1), "r"(V1.s2), "r"(V1.s3),
          "r"(V1.s4), "r"(V1.s5), "r"(V1.s6), "r"(V1.s7));

// V0 ^= T
    asm("{\n"
      "xor.b32 %0, %0, %8;\n"
      "xor.b32 %1, %1, %9;\n"
      "xor.b32 %2, %2, %10;\n"
      "xor.b32 %3, %3, %11;\n"
      "xor.b32 %4, %4, %12;\n"
      "xor.b32 %5, %5, %13;\n"
      "xor.b32 %6, %6, %14;\n"
      "xor.b32 %7, %7, %15;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s1), "+r"(V0.s2), "+r"(V0.s3),
          "+r"(V0.s4), "+r"(V0.s5), "+r"(V0.s6), "+r"(V0.s7)
        : "r"(T.s0), "r"(T.s1), "r"(T.s2), "r"(T.s3),
          "r"(T.s4), "r"(T.s5), "r"(T.s6), "r"(T.s7));

    V1 = BLAKE2s_IV;
    T = V0;
    V1.s4 ^= 128;
    V1.s6 = ~V1.s6;


//    G(V0.s0, V0.s4, V1.s0, V1.s4, inout[0], inout[1]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s4), "+r"(V1.s0), "+r"(V1.s4)
        : "r"(inout[0]), "r"(inout[1]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s5, V1.s1, V1.s5, inout[2], inout[3]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s5), "+r"(V1.s1), "+r"(V1.s5)
        : "r"(inout[2]), "r"(inout[3]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s6, V1.s2, V1.s6, inout[4], inout[5]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s6), "+r"(V1.s2), "+r"(V1.s6)
        : "r"(inout[4]), "r"(inout[5]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s3, V0.s7, V1.s3, V1.s7, inout[6], inout[7]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s7), "+r"(V1.s3), "+r"(V1.s7)
        : "r"(inout[6]), "r"(inout[7]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s0, V0.s5, V1.s2, V1.s7, inout[8], inout[9]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s5), "+r"(V1.s2), "+r"(V1.s7)
        : "r"(inout[8]), "r"(inout[9]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s6, V1.s3, V1.s4, inout[10], inout[11]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s6), "+r"(V1.s3), "+r"(V1.s4)
        : "r"(inout[10]), "r"(inout[11]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s7, V1.s0, V1.s5, inout[12], inout[13]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s7), "+r"(V1.s0), "+r"(V1.s5)
        : "r"(inout[12]), "r"(inout[13]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s3, V0.s4, V1.s1, V1.s6, inout[14], inout[15]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s4), "+r"(V1.s1), "+r"(V1.s6)
        : "r"(inout[14]), "r"(inout[15]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));


//    G(V0.s0, V0.s4, V1.s0, V1.s4, inout[14], inout[10]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s4), "+r"(V1.s0), "+r"(V1.s4)
        : "r"(inout[14]), "r"(inout[10]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s5, V1.s1, V1.s5, inout[4], inout[8]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s5), "+r"(V1.s1), "+r"(V1.s5)
        : "r"(inout[4]), "r"(inout[8]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s6, V1.s2, V1.s6, inout[9], inout[15]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s6), "+r"(V1.s2), "+r"(V1.s6)
        : "r"(inout[9]), "r"(inout[15]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s3, V0.s7, V1.s3, V1.s7, inout[13], inout[6]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s7), "+r"(V1.s3), "+r"(V1.s7)
        : "r"(inout[13]), "r"(inout[6]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s0, V0.s5, V1.s2, V1.s7, inout[1], inout[12]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s5), "+r"(V1.s2), "+r"(V1.s7)
        : "r"(inout[1]), "r"(inout[12]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s6, V1.s3, V1.s4, inout[0], inout[2]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s6), "+r"(V1.s3), "+r"(V1.s4)
        : "r"(inout[0]), "r"(inout[2]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s7, V1.s0, V1.s5, inout[11], inout[7]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s7), "+r"(V1.s0), "+r"(V1.s5)
        : "r"(inout[11]), "r"(inout[7]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s3, V0.s4, V1.s1, V1.s6, inout[5], inout[3]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s4), "+r"(V1.s1), "+r"(V1.s6)
        : "r"(inout[5]), "r"(inout[3]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));


//    G(V0.s0, V0.s4, V1.s0, V1.s4, inout[11], inout[8]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s4), "+r"(V1.s0), "+r"(V1.s4)
        : "r"(inout[11]), "r"(inout[8]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s5, V1.s1, V1.s5, inout[12], inout[0]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s5), "+r"(V1.s1), "+r"(V1.s5)
        : "r"(inout[12]), "r"(inout[0]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s6, V1.s2, V1.s6, inout[5], inout[2]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s6), "+r"(V1.s2), "+r"(V1.s6)
        : "r"(inout[5]), "r"(inout[2]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s3, V0.s7, V1.s3, V1.s7, inout[15], inout[13]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s7), "+r"(V1.s3), "+r"(V1.s7)
        : "r"(inout[15]), "r"(inout[13]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s0, V0.s5, V1.s2, V1.s7, inout[10], inout[14]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s5), "+r"(V1.s2), "+r"(V1.s7)
        : "r"(inout[10]), "r"(inout[14]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s6, V1.s3, V1.s4, inout[3], inout[6]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s6), "+r"(V1.s3), "+r"(V1.s4)
        : "r"(inout[3]), "r"(inout[6]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s7, V1.s0, V1.s5, inout[7], inout[1]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s7), "+r"(V1.s0), "+r"(V1.s5)
        : "r"(inout[7]), "r"(inout[1]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s3, V0.s4, V1.s1, V1.s6, inout[9], inout[4]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s4), "+r"(V1.s1), "+r"(V1.s6)
        : "r"(inout[9]), "r"(inout[4]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));


//    G(V0.s0, V0.s4, V1.s0, V1.s4, inout[7], inout[9]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s4), "+r"(V1.s0), "+r"(V1.s4)
        : "r"(inout[7]), "r"(inout[9]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s5, V1.s1, V1.s5, inout[3], inout[1]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s5), "+r"(V1.s1), "+r"(V1.s5)
        : "r"(inout[3]), "r"(inout[1]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s6, V1.s2, V1.s6, inout[13], inout[12]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s6), "+r"(V1.s2), "+r"(V1.s6)
        : "r"(inout[13]), "r"(inout[12]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s3, V0.s7, V1.s3, V1.s7, inout[11], inout[14]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s7), "+r"(V1.s3), "+r"(V1.s7)
        : "r"(inout[11]), "r"(inout[14]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s0, V0.s5, V1.s2, V1.s7, inout[2], inout[6]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s5), "+r"(V1.s2), "+r"(V1.s7)
        : "r"(inout[2]), "r"(inout[6]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s6, V1.s3, V1.s4, inout[5], inout[10]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s6), "+r"(V1.s3), "+r"(V1.s4)
        : "r"(inout[5]), "r"(inout[10]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s7, V1.s0, V1.s5, inout[4], inout[0]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s7), "+r"(V1.s0), "+r"(V1.s5)
        : "r"(inout[4]), "r"(inout[0]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s3, V0.s4, V1.s1, V1.s6, inout[15], inout[8]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s4), "+r"(V1.s1), "+r"(V1.s6)
        : "r"(inout[15]), "r"(inout[8]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));


//    G(V0.s0, V0.s4, V1.s0, V1.s4, inout[9], inout[0]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s4), "+r"(V1.s0), "+r"(V1.s4)
        : "r"(inout[9]), "r"(inout[0]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s5, V1.s1, V1.s5, inout[5], inout[7]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s5), "+r"(V1.s1), "+r"(V1.s5)
        : "r"(inout[5]), "r"(inout[7]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s6, V1.s2, V1.s6, inout[2], inout[4]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s6), "+r"(V1.s2), "+r"(V1.s6)
        : "r"(inout[2]), "r"(inout[4]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s3, V0.s7, V1.s3, V1.s7, inout[10], inout[15]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s7), "+r"(V1.s3), "+r"(V1.s7)
        : "r"(inout[10]), "r"(inout[15]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s0, V0.s5, V1.s2, V1.s7, inout[14], inout[1]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s5), "+r"(V1.s2), "+r"(V1.s7)
        : "r"(inout[14]), "r"(inout[1]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s6, V1.s3, V1.s4, inout[11], inout[12]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s6), "+r"(V1.s3), "+r"(V1.s4)
        : "r"(inout[11]), "r"(inout[12]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s7, V1.s0, V1.s5, inout[6], inout[8]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s7), "+r"(V1.s0), "+r"(V1.s5)
        : "r"(inout[6]), "r"(inout[8]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s3, V0.s4, V1.s1, V1.s6, inout[3], inout[13]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s4), "+r"(V1.s1), "+r"(V1.s6)
        : "r"(inout[3]), "r"(inout[13]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));


//    G(V0.s0, V0.s4, V1.s0, V1.s4, inout[2], inout[12]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s4), "+r"(V1.s0), "+r"(V1.s4)
        : "r"(inout[2]), "r"(inout[12]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s5, V1.s1, V1.s5, inout[6], inout[10]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s5), "+r"(V1.s1), "+r"(V1.s5)
        : "r"(inout[6]), "r"(inout[10]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s6, V1.s2, V1.s6, inout[0], inout[11]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s6), "+r"(V1.s2), "+r"(V1.s6)
        : "r"(inout[0]), "r"(inout[11]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s3, V0.s7, V1.s3, V1.s7, inout[8], inout[3]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s7), "+r"(V1.s3), "+r"(V1.s7)
        : "r"(inout[8]), "r"(inout[3]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s0, V0.s5, V1.s2, V1.s7, inout[4], inout[13]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s5), "+r"(V1.s2), "+r"(V1.s7)
        : "r"(inout[4]), "r"(inout[13]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s6, V1.s3, V1.s4, inout[7], inout[5]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s6), "+r"(V1.s3), "+r"(V1.s4)
        : "r"(inout[7]), "r"(inout[5]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s7, V1.s0, V1.s5, inout[15], inout[14]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s7), "+r"(V1.s0), "+r"(V1.s5)
        : "r"(inout[15]), "r"(inout[14]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s3, V0.s4, V1.s1, V1.s6, inout[1], inout[9]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s4), "+r"(V1.s1), "+r"(V1.s6)
        : "r"(inout[1]), "r"(inout[9]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));


//    G(V0.s0, V0.s4, V1.s0, V1.s4, inout[12], inout[5]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s4), "+r"(V1.s0), "+r"(V1.s4)
        : "r"(inout[12]), "r"(inout[5]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s5, V1.s1, V1.s5, inout[1], inout[15]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s5), "+r"(V1.s1), "+r"(V1.s5)
        : "r"(inout[1]), "r"(inout[15]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s6, V1.s2, V1.s6, inout[14], inout[13]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s6), "+r"(V1.s2), "+r"(V1.s6)
        : "r"(inout[14]), "r"(inout[13]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s3, V0.s7, V1.s3, V1.s7, inout[4], inout[10]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s7), "+r"(V1.s3), "+r"(V1.s7)
        : "r"(inout[4]), "r"(inout[10]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s0, V0.s5, V1.s2, V1.s7, inout[0], inout[7]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s5), "+r"(V1.s2), "+r"(V1.s7)
        : "r"(inout[0]), "r"(inout[7]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s6, V1.s3, V1.s4, inout[6], inout[3]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s6), "+r"(V1.s3), "+r"(V1.s4)
        : "r"(inout[6]), "r"(inout[3]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s7, V1.s0, V1.s5, inout[9], inout[2]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s7), "+r"(V1.s0), "+r"(V1.s5)
        : "r"(inout[9]), "r"(inout[2]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s3, V0.s4, V1.s1, V1.s6, inout[8], inout[11]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s4), "+r"(V1.s1), "+r"(V1.s6)
        : "r"(inout[8]), "r"(inout[11]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));


//    G(V0.s0, V0.s4, V1.s0, V1.s4, inout[13], inout[11]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s4), "+r"(V1.s0), "+r"(V1.s4)
        : "r"(inout[13]), "r"(inout[11]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s5, V1.s1, V1.s5, inout[7], inout[14]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s5), "+r"(V1.s1), "+r"(V1.s5)
        : "r"(inout[7]), "r"(inout[14]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s6, V1.s2, V1.s6, inout[12], inout[1]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s6), "+r"(V1.s2), "+r"(V1.s6)
        : "r"(inout[12]), "r"(inout[1]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s3, V0.s7, V1.s3, V1.s7, inout[3], inout[9]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s7), "+r"(V1.s3), "+r"(V1.s7)
        : "r"(inout[3]), "r"(inout[9]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s0, V0.s5, V1.s2, V1.s7, inout[5], inout[0]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s5), "+r"(V1.s2), "+r"(V1.s7)
        : "r"(inout[5]), "r"(inout[0]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s6, V1.s3, V1.s4, inout[15], inout[4]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s6), "+r"(V1.s3), "+r"(V1.s4)
        : "r"(inout[15]), "r"(inout[4]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s7, V1.s0, V1.s5, inout[8], inout[6]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s7), "+r"(V1.s0), "+r"(V1.s5)
        : "r"(inout[8]), "r"(inout[6]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s3, V0.s4, V1.s1, V1.s6, inout[2], inout[10]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s4), "+r"(V1.s1), "+r"(V1.s6)
        : "r"(inout[2]), "r"(inout[10]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));


//    G(V0.s0, V0.s4, V1.s0, V1.s4, inout[6], inout[15]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s4), "+r"(V1.s0), "+r"(V1.s4)
        : "r"(inout[6]), "r"(inout[15]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s5, V1.s1, V1.s5, inout[14], inout[9]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s5), "+r"(V1.s1), "+r"(V1.s5)
        : "r"(inout[14]), "r"(inout[9]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s6, V1.s2, V1.s6, inout[11], inout[3]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s6), "+r"(V1.s2), "+r"(V1.s6)
        : "r"(inout[11]), "r"(inout[3]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s3, V0.s7, V1.s3, V1.s7, inout[0], inout[8]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s7), "+r"(V1.s3), "+r"(V1.s7)
        : "r"(inout[0]), "r"(inout[8]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s0, V0.s5, V1.s2, V1.s7, inout[12], inout[2]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s5), "+r"(V1.s2), "+r"(V1.s7)
        : "r"(inout[12]), "r"(inout[2]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s6, V1.s3, V1.s4, inout[13], inout[7]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s6), "+r"(V1.s3), "+r"(V1.s4)
        : "r"(inout[13]), "r"(inout[7]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s7, V1.s0, V1.s5, inout[1], inout[4]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s7), "+r"(V1.s0), "+r"(V1.s5)
        : "r"(inout[1]), "r"(inout[4]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s3, V0.s4, V1.s1, V1.s6, inout[10], inout[5]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s4), "+r"(V1.s1), "+r"(V1.s6)
        : "r"(inout[10]), "r"(inout[5]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));


//    G(V0.s0, V0.s4, V1.s0, V1.s4, inout[10], inout[2]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s4), "+r"(V1.s0), "+r"(V1.s4)
        : "r"(inout[10]), "r"(inout[2]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s5, V1.s1, V1.s5, inout[8], inout[4]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s5), "+r"(V1.s1), "+r"(V1.s5)
        : "r"(inout[8]), "r"(inout[4]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s6, V1.s2, V1.s6, inout[7], inout[6]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s6), "+r"(V1.s2), "+r"(V1.s6)
        : "r"(inout[7]), "r"(inout[6]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s3, V0.s7, V1.s3, V1.s7, inout[1], inout[5]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s7), "+r"(V1.s3), "+r"(V1.s7)
        : "r"(inout[1]), "r"(inout[5]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s0, V0.s5, V1.s2, V1.s7, inout[15], inout[11]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s5), "+r"(V1.s2), "+r"(V1.s7)
        : "r"(inout[15]), "r"(inout[11]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s1, V0.s6, V1.s3, V1.s4, inout[9], inout[14]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s1), "+r"(V0.s6), "+r"(V1.s3), "+r"(V1.s4)
        : "r"(inout[9]), "r"(inout[14]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s2, V0.s7, V1.s0, V1.s5, inout[3], inout[12]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s2), "+r"(V0.s7), "+r"(V1.s0), "+r"(V1.s5)
        : "r"(inout[3]), "r"(inout[12]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

//    G(V0.s3, V0.s4, V1.s1, V1.s6, inout[13], inout[0]);
    asm("{\n"
      "add.u32 %0, %0, %4;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %6;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %7;\n"
      "add.u32 %0, %0, %5;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %10, %8;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %9;\n"
    "}" : "+r"(V0.s3), "+r"(V0.s4), "+r"(V1.s1), "+r"(V1.s6)
        : "r"(inout[13]), "r"(inout[0]), "r"(c16), "r"(c20), "r"(c24), "r"(c25), "r"(c0));

// V0 ^= V1
    asm("{\n"
      "xor.b32 %0, %0, %8;\n"
      "xor.b32 %1, %1, %9;\n"
      "xor.b32 %2, %2, %10;\n"
      "xor.b32 %3, %3, %11;\n"
      "xor.b32 %4, %4, %12;\n"
      "xor.b32 %5, %5, %13;\n"
      "xor.b32 %6, %6, %14;\n"
      "xor.b32 %7, %7, %15;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s1), "+r"(V0.s2), "+r"(V0.s3),
          "+r"(V0.s4), "+r"(V0.s5), "+r"(V0.s6), "+r"(V0.s7)
        : "r"(V1.s0), "r"(V1.s1), "r"(V1.s2), "r"(V1.s3),
          "r"(V1.s4), "r"(V1.s5), "r"(V1.s6), "r"(V1.s7));

// V0 ^= T
    asm("{\n"
      "xor.b32 %0, %0, %8;\n"
      "xor.b32 %1, %1, %9;\n"
      "xor.b32 %2, %2, %10;\n"
      "xor.b32 %3, %3, %11;\n"
      "xor.b32 %4, %4, %12;\n"
      "xor.b32 %5, %5, %13;\n"
      "xor.b32 %6, %6, %14;\n"
      "xor.b32 %7, %7, %15;\n"
    "}" : "+r"(V0.s0), "+r"(V0.s1), "+r"(V0.s2), "+r"(V0.s3),
          "+r"(V0.s4), "+r"(V0.s5), "+r"(V0.s6), "+r"(V0.s7)
        : "r"(T.s0), "r"(T.s1), "r"(T.s2), "r"(T.s3),
          "r"(T.s4), "r"(T.s5), "r"(T.s6), "r"(T.s7));

    ((uint8 *) out)[0] = V0;
}

static __forceinline__ __host__
void blake2s_host(uint *inout, const uint *key) {
    uint8 V0, V1, T;
    uint i, idx;

    V1 = BLAKE2s_IV_host;
    V0 = BLAKE2s_IV_host;
    V0.s0 ^= 0x01012020;

    T = V0;

    V1.s4 ^= 64;

    for(i = 0; i < 10; i++) {
        Ghost(i, 0x00, V0.s0, V0.s4, V1.s0, V1.s4, key);
        Ghost(i, 0x02, V0.s1, V0.s5, V1.s1, V1.s5, key);
        Ghost(i, 0x04, V0.s2, V0.s6, V1.s2, V1.s6, key);
        Ghost(i, 0x06, V0.s3, V0.s7, V1.s3, V1.s7, key);
        Ghost(i, 0x08, V0.s0, V0.s5, V1.s2, V1.s7, key);
        Ghost(i, 0x0A, V0.s1, V0.s6, V1.s3, V1.s4, key);
        Ghost(i, 0x0C, V0.s2, V0.s7, V1.s0, V1.s5, key);
        Ghost(i, 0x0E, V0.s3, V0.s4, V1.s1, V1.s6, key);
    }

    V0 ^= V1;
    V0 ^= T;
    V1 = BLAKE2s_IV_host;
    T = V0;
    V1.s4 ^= 128;
    V1.s6 = ~V1.s6;

    for(i = 0; i < 10; i++) {
        Ghost(i, 0x00, V0.s0, V0.s4, V1.s0, V1.s4, inout);
        Ghost(i, 0x02, V0.s1, V0.s5, V1.s1, V1.s5, inout);
        Ghost(i, 0x04, V0.s2, V0.s6, V1.s2, V1.s6, inout);
        Ghost(i, 0x06, V0.s3, V0.s7, V1.s3, V1.s7, inout);
        Ghost(i, 0x08, V0.s0, V0.s5, V1.s2, V1.s7, inout);
        Ghost(i, 0x0A, V0.s1, V0.s6, V1.s3, V1.s4, inout);
        Ghost(i, 0x0C, V0.s2, V0.s7, V1.s0, V1.s5, inout);
        Ghost(i, 0x0E, V0.s3, V0.s4, V1.s1, V1.s6, inout);
    }

    V0 ^= V1;
    V0 ^= T;

    ((uint8 *) inout)[0] = V0;
}


#define TPB 128
#define TPB_MIX_MODE1 128
#define TPB_MIX_MODE2 512
#define TPB_MIX_MODE3 128

__global__ __launch_bounds__(TPB, 1)
void neoscrypt_gpu_hash_start(uint startNonce) {
    const uint thrid = blockDim.x * blockIdx.x + threadIdx.x;
    const uint shiftTr = thrid * 8;
    const uint nonce = thrid + startNonce;
    uint i, j;

    uint __align__(16) input[16];
    uint __align__(16) key[16];

    __shared__ __align__(16) uint s_data[64 * TPB];
    uint *B = (uint *) &s_data[threadIdx.x * 64];

    /* SASS LD.E.128 and ST.E.128 expected */
    ((uint64 *) B)[0] = ((uint64 *) c_data)[0];

    asm("ldu.v4.b32 {%0, %1, %2, %3}, [%4];"
      : "=r"(input[0]), "=r"(input[1]), "=r"(input[2]), "=r"(input[3])
      : __MEM_PTR(&input_init[0]));
    asm("ldu.v4.b32 {%0, %1, %2, %3}, [%4];"
      : "=r"(input[4]), "=r"(input[5]), "=r"(input[6]), "=r"(input[7])
      : __MEM_PTR(&input_init[4]));
    asm("ldu.v4.b32 {%0, %1, %2, %3}, [%4];"
      : "=r"(input[8]), "=r"(input[9]), "=r"(input[10]), "=r"(input[11])
      : __MEM_PTR(&input_init[8]));
    asm("ldu.v4.b32 {%0, %1, %2, %3}, [%4];"
      : "=r"(input[12]), "=r"(input[13]), "=r"(input[14]), "=r"(input[15])
      : __MEM_PTR(&input_init[12]));

    asm("ldu.v4.b32 {%0, %1, %2, %3}, [%4];"
      : "=r"(key[0]), "=r"(key[1]), "=r"(key[2]), "=r"(key[3])
      : __MEM_PTR(&key_init[0]));
    asm("ldu.v4.b32 {%0, %1, %2, %3}, [%4];"
      : "=r"(key[4]), "=r"(key[5]), "=r"(key[6]), "=r"(key[7])
      : __MEM_PTR(&key_init[4]));

    key[8]  = 0; key[9]  = 0; key[10] = 0; key[11] = 0;
    key[12] = 0; key[13] = 0; key[14] = 0; key[15] = 0;

    B[19] = nonce;
    B[39] = nonce;
    B[59] = nonce;

    uint temp[9];
    uint bufptr, qbuf, bitbuf, shift, a, b;
    const uint c63 = 0x3F;

    for(i = 0; i < 31; i++) {

        bufptr = input[0] + input[1] + input[2] + input[3] +
                 input[4] + input[5] + input[6] + input[7];
        bufptr += (input[0] >> 8)  + (input[1] >> 8) +
                  (input[2] >> 8)  + (input[3] >> 8) +
                  (input[4] >> 8)  + (input[5] >> 8) +
                  (input[6] >> 8)  + (input[7] >> 8);
        bufptr += (input[0] >> 16) + (input[1] >> 16) +
                  (input[2] >> 16) + (input[3] >> 16) +
                  (input[4] >> 16) + (input[5] >> 16) +
                  (input[6] >> 16) + (input[7] >> 16);
        bufptr += (input[0] >> 24) + (input[1] >> 24) +
                  (input[2] >> 24) + (input[3] >> 24) +
                  (input[4] >> 24) + (input[5] >> 24) +
                  (input[6] >> 24) + (input[7] >> 24);
        bufptr &= 0xFF;

        qbuf = bufptr >> 2;
        bitbuf = (bufptr & 0x03) << 3;

        shift = 32 - bitbuf;

    asm("ld.b32 %0, [%1];" : "=r"(temp[0]) : __MEM_PTR(&B[qbuf & c63]));
    asm("ld.b32 %0, [%1];" : "=r"(temp[1]) : __MEM_PTR(&B[(qbuf + 1) & c63]));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(a) : "r"(0), "r"(input[0]), "r"(shift));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(b) : "r"(input[0]), "r"(input[1]), "r"(shift));
    asm("xor.b32 %0, %0, %1;" : "+r"(temp[0]) : "r"(a));
    asm("xor.b32 %0, %0, %1;" : "+r"(temp[1]) : "r"(b));
    asm("st.b32 [%1], %0;" : : "r"(temp[0]), __MEM_PTR(&B[qbuf & c63]));

    asm("ld.b32 %0, [%1];" : "=r"(temp[2]) : __MEM_PTR(&B[(qbuf + 2) & c63]));
    asm("ld.b32 %0, [%1];" : "=r"(temp[3]) : __MEM_PTR(&B[(qbuf + 3) & c63]));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(a) : "r"(input[1]), "r"(input[2]), "r"(shift));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(b) : "r"(input[2]), "r"(input[3]), "r"(shift));
    asm("xor.b32 %0, %0, %1;" : "+r"(temp[2]) : "r"(a));
    asm("xor.b32 %0, %0, %1;" : "+r"(temp[3]) : "r"(b));
    asm("st.b32 [%1], %0;" : : "r"(temp[1]), __MEM_PTR(&B[(qbuf + 1) & c63]));

    asm("ld.b32 %0, [%1];" : "=r"(temp[4]) : __MEM_PTR(&B[(qbuf + 4) & c63]));
    asm("ld.b32 %0, [%1];" : "=r"(temp[5]) : __MEM_PTR(&B[(qbuf + 5) & c63]));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(a) : "r"(input[3]), "r"(input[4]), "r"(shift));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(b) : "r"(input[4]), "r"(input[5]), "r"(shift));
    asm("xor.b32 %0, %0, %1;" : "+r"(temp[4]) : "r"(a));
    asm("xor.b32 %0, %0, %1;" : "+r"(temp[5]) : "r"(b));
    asm("st.b32 [%1], %0;" : : "r"(temp[2]), __MEM_PTR(&B[(qbuf + 2) & c63]));

    asm("ld.b32 %0, [%1];" : "=r"(temp[6]) : __MEM_PTR(&B[(qbuf + 6) & c63]));
    asm("ld.b32 %0, [%1];" : "=r"(temp[7]) : __MEM_PTR(&B[(qbuf + 7) & c63]));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(a) : "r"(input[5]), "r"(input[6]), "r"(shift));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(b) : "r"(input[6]), "r"(input[7]), "r"(shift));
    asm("xor.b32 %0, %0, %1;" : "+r"(temp[6]) : "r"(a));
    asm("xor.b32 %0, %0, %1;" : "+r"(temp[7]) : "r"(b));
    asm("st.b32 [%1], %0;" : : "r"(temp[3]), __MEM_PTR(&B[(qbuf + 3) & c63]));

    asm("ld.b32 %0, [%1];" : "=r"(temp[8]) : __MEM_PTR(&B[(qbuf + 8) & c63]));
    asm("shr.b32 %0, %1, %2;" : "=r"(a) : "r"(input[7]), "r"(shift));
    asm("xor.b32 %0, %0, %1;" : "+r"(temp[8]) : "r"(a));
    asm("st.b32 [%1], %0;" : : "r"(temp[4]), __MEM_PTR(&B[(qbuf + 4) & c63]));
    asm("st.b32 [%1], %0;" : : "r"(temp[5]), __MEM_PTR(&B[(qbuf + 5) & c63]));
    asm("st.b32 [%1], %0;" : : "r"(temp[6]), __MEM_PTR(&B[(qbuf + 6) & c63]));
    asm("st.b32 [%1], %0;" : : "r"(temp[7]), __MEM_PTR(&B[(qbuf + 7) & c63]));
    asm("st.b32 [%1], %0;" : : "r"(temp[8]), __MEM_PTR(&B[(qbuf + 8) & c63]));

        a = c_data[qbuf & c63];
        for(j = 0; j < 16; j += 2) {
            b = c_data[(qbuf + j + 1) & c63];
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(input[j]) : "r"(a), "r"(b), "r"(bitbuf));
            a = c_data[(qbuf + j + 2) & c63];
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(input[j + 1]) : "r"(b), "r"(a), "r"(bitbuf));

    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(key[j >> 1])
      : "r"(temp[j >> 1]), "r"(temp[(j >> 1) + 1]), "r"(bitbuf));
        }

        if(qbuf < 60) {
            uint noncepos = 19 - qbuf % 20;
            if(noncepos <= 16) {
                if(noncepos != 0)
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(input[noncepos - 1])
      : "r"(c_data[18]), "r"(nonce), "r"(bitbuf));
                if(noncepos !=16)
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(input[noncepos])
      : "r"(nonce), "r"(c_data[20]), "r"(bitbuf));
            }
        }

        blake2s(input, input, key);

    }

    bufptr = input[0] + input[1] + input[2] + input[3] +
             input[4] + input[5] + input[6] + input[7];
    bufptr += (input[0] >> 8)  + (input[1] >> 8) +
              (input[2] >> 8)  + (input[3] >> 8) +
              (input[4] >> 8)  + (input[5] >> 8) +
              (input[6] >> 8)  + (input[7] >> 8);
    bufptr += (input[0] >> 16) + (input[1] >> 16) +
              (input[2] >> 16) + (input[3] >> 16) +
              (input[4] >> 16) + (input[5] >> 16) +
              (input[6] >> 16) + (input[7] >> 16);
    bufptr += (input[0] >> 24) + (input[1] >> 24) +
              (input[2] >> 24) + (input[3] >> 24) +
              (input[4] >> 24) + (input[5] >> 24) +
              (input[6] >> 24) + (input[7] >> 24);
    bufptr &= 0xFF;

    qbuf = bufptr >> 2;
    bitbuf = (bufptr & 0x03) << 3;

    uint c, x0, x1, x2, x3;

    asm("ldu.v4.b32 {%0, %1, %2, %3}, [%4];" : "=r"(x0), "=r"(x1), "=r"(x2), "=r"(x3)
      : __MEM_PTR(&c_data[0]));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf) & c63]));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 1) & c63]));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 2) & c63]));
    asm("xor.b32 %0, %0, %1;" : "+r"(x0) : "r"(c));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[0]) : "r"(x0));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 3) & c63]));
    asm("xor.b32 %0, %0, %1;" : "+r"(x1) : "r"(c));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[1]) : "r"(x1));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 4) & c63]));
    asm("xor.b32 %0, %0, %1;" : "+r"(x2) : "r"(c));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[2]) : "r"(x2));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 5) & c63]));
    asm("xor.b32 %0, %0, %1;" : "+r"(x3) : "r"(c));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[3]) : "r"(x3));

    asm("ldu.v4.b32 {%0, %1, %2, %3}, [%4];" : "=r"(x0), "=r"(x1), "=r"(x2), "=r"(x3)
      : __MEM_PTR(&c_data[4]));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 6) & c63]));
    asm("xor.b32 %0, %0, %1;" : "+r"(x0) : "r"(c));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[4]) : "r"(x0));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 7) & c63]));
    asm("xor.b32 %0, %0, %1;" : "+r"(x1) : "r"(c));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[5]) : "r"(x1));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 8) & c63]));
    asm("xor.b32 %0, %0, %1;" : "+r"(x2) : "r"(c));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[6]) : "r"(x2));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 9) & c63]));
    asm("xor.b32 %0, %0, %1;" : "+r"(x3) : "r"(c));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[7]) : "r"(x3));

    asm("ldu.v4.b32 {%0, %1, %2, %3}, [%4];" : "=r"(x0), "=r"(x1), "=r"(x2), "=r"(x3)
      : __MEM_PTR(&c_data[8]));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 10) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[8]) : "r"(c), "r"(x0));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 11) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[9]) : "r"(c), "r"(x1));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 12) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[10]) : "r"(c), "r"(x2));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 13) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[11]) : "r"(c), "r"(x3));

    asm("ldu.v4.b32 {%0, %1, %2, %3}, [%4];" : "=r"(x0), "=r"(x1), "=r"(x2), "=r"(x3)
      : __MEM_PTR(&c_data[12]));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 14) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[12]) : "r"(c), "r"(x0));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 15) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[13]) : "r"(c), "r"(x1));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 16) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[14]) : "r"(c), "r"(x2));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 17) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[15]) : "r"(c), "r"(x3));

    asm("ldu.v4.b32 {%0, %1, %2, %3}, [%4];" : "=r"(x0), "=r"(x1), "=r"(x2), "=r"(x3)
      : __MEM_PTR(&c_data[16]));

    ((ulong8 *) (Input + shiftTr))[0] = ((ulong8 *) input)[0];

    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 18) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[0]) : "r"(c), "r"(x0));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 19) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[1]) : "r"(c), "r"(x1));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 20) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[2]) : "r"(c), "r"(x2));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 21) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[3]) : "r"(c), "r"(nonce));

    asm("ldu.v4.b32 {%0, %1, %2, %3}, [%4];" : "=r"(x0), "=r"(x1), "=r"(x2), "=r"(x3)
      : __MEM_PTR(&c_data[20]));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 22) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[4]) : "r"(c), "r"(x0));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 23) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[5]) : "r"(c), "r"(x1));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 24) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[6]) : "r"(c), "r"(x2));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 25) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[7]) : "r"(c), "r"(x3));

    asm("ldu.v4.b32 {%0, %1, %2, %3}, [%4];" : "=r"(x0), "=r"(x1), "=r"(x2), "=r"(x3)
      : __MEM_PTR(&c_data[24]));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 26) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[8]) : "r"(c), "r"(x0));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 27) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[9]) : "r"(c), "r"(x1));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 28) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[10]) : "r"(c), "r"(x2));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 29) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[11]) : "r"(c), "r"(x3));

    asm("ldu.v4.b32 {%0, %1, %2, %3}, [%4];" : "=r"(x0), "=r"(x1), "=r"(x2), "=r"(x3)
      : __MEM_PTR(&c_data[28]));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 30) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[12]) : "r"(c), "r"(x0));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 31) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[13]) : "r"(c), "r"(x1));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 32) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[14]) : "r"(c), "r"(x2));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 33) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[15]) : "r"(c), "r"(x3));

    asm("ldu.v4.b32 {%0, %1, %2, %3}, [%4];" : "=r"(x0), "=r"(x1), "=r"(x2), "=r"(x3)
      : __MEM_PTR(&c_data[32]));

    ((ulong8 *) (Input + shiftTr))[1] = ((ulong8 *) input)[0];

    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 34) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[0]) : "r"(c), "r"(x0));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 35) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[1]) : "r"(c), "r"(x1));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 36) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[2]) : "r"(c), "r"(x2));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 37) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[3]) : "r"(c), "r"(x3));

    asm("ldu.v4.b32 {%0, %1, %2, %3}, [%4];" : "=r"(x0), "=r"(x1), "=r"(x2), "=r"(x3)
      : __MEM_PTR(&c_data[36]));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 38) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[4]) : "r"(c), "r"(x0));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 39) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[5]) : "r"(c), "r"(x1));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 40) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[6]) : "r"(c), "r"(x2));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 41) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[7]) : "r"(c), "r"(nonce));

    asm("ldu.v4.b32 {%0, %1, %2, %3}, [%4];" : "=r"(x0), "=r"(x1), "=r"(x2), "=r"(x3)
      : __MEM_PTR(&c_data[40]));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 42) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[8]) : "r"(c), "r"(x0));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 43) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[9]) : "r"(c), "r"(x1));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 44) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[10]) : "r"(c), "r"(x2));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 45) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[11]) : "r"(c), "r"(x3));

    asm("ldu.v4.b32 {%0, %1, %2, %3}, [%4];" : "=r"(x0), "=r"(x1), "=r"(x2), "=r"(x3)
      : __MEM_PTR(&c_data[44]));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 46) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[12]) : "r"(c), "r"(x0));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 47) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[13]) : "r"(c), "r"(x1));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 48) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[14]) : "r"(c), "r"(x2));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 49) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[15]) : "r"(c), "r"(x3));

    asm("ldu.v4.b32 {%0, %1, %2, %3}, [%4];" : "=r"(x0), "=r"(x1), "=r"(x2), "=r"(x3)
      : __MEM_PTR(&c_data[48]));

    ((ulong8 *) (Input + shiftTr))[2] = ((ulong8 *) input)[0];

    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 50) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[0]) : "r"(c), "r"(x0));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 51) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[1]) : "r"(c), "r"(x1));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 52) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[2]) : "r"(c), "r"(x2));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 53) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[3]) : "r"(c), "r"(x3));

    asm("ldu.v4.b32 {%0, %1, %2, %3}, [%4];" : "=r"(x0), "=r"(x1), "=r"(x2), "=r"(x3)
      : __MEM_PTR(&c_data[52]));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 54) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[4]) : "r"(c), "r"(x0));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 55) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[5]) : "r"(c), "r"(x1));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 56) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[6]) : "r"(c), "r"(x2));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 57) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[7]) : "r"(c), "r"(x3));

    asm("ldu.v4.b32 {%0, %1, %2, %3}, [%4];" : "=r"(x0), "=r"(x1), "=r"(x2), "=r"(x3)
      : __MEM_PTR(&c_data[56]));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 58) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[8]) : "r"(c), "r"(x0));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 59) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[9]) : "r"(c), "r"(x1));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 60) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[10]) : "r"(c), "r"(x2));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 61) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[11]) : "r"(c), "r"(nonce));

    asm("ldu.v4.b32 {%0, %1, %2, %3}, [%4];" : "=r"(x0), "=r"(x1), "=r"(x2), "=r"(x3)
      : __MEM_PTR(&c_data[60]));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 62) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[12]) : "r"(c), "r"(x0));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(b) : __MEM_PTR(&B[(qbuf + 63) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[13]) : "r"(c), "r"(x1));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(a), "r"(b), "r"(bitbuf));
    asm("ld.b32 %0, [%1];" : "=r"(a) : __MEM_PTR(&B[(qbuf + 64) & c63]));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[14]) : "r"(c), "r"(x2));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(c) : "r"(b), "r"(a), "r"(bitbuf));
    asm("xor.b32 %0, %1, %2;" : "=r"(input[15]) : "r"(c), "r"(x3));

    ((ulong8 *) (Input + shiftTr))[3] = ((ulong8 *) input)[0];
}

__global__ __launch_bounds__(TPB, 1)
void neoscrypt_gpu_hash_end(uint startNonce, uint *nonceVector) {
    const uint thrid = blockDim.x * blockIdx.x + threadIdx.x;
    const uint shiftTr = thrid * 8;
    const uint nonce = thrid + startNonce;
    uint i, j;

    const uint data7  = c_data[7];

    uint __align__(16) input[16];
    uint __align__(16) key[16];

    __shared__ __align__(16) uint s_data[64 * TPB];
    uint *B = (uint *) &s_data[threadIdx.x * 64];

    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4];"
      : "=r"(input[0]), "=r"(input[1]), "=r"(input[2]), "=r"(input[3])
      : __MEM_PTR(&(Tr + shiftTr)[1]));
    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4 + 16];"
      : "=r"(input[4]), "=r"(input[5]), "=r"(input[6]), "=r"(input[7])
      : __MEM_PTR(&(Tr + shiftTr)[1]));
    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4];"
      : "=r"(input[8]), "=r"(input[9]), "=r"(input[10]), "=r"(input[11])
      : __MEM_PTR(&(Tr2 + shiftTr)[1]));
    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4 + 16];"
      : "=r"(input[12]), "=r"(input[13]), "=r"(input[14]), "=r"(input[15])
      : __MEM_PTR(&(Tr2 + shiftTr)[1]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[0]) : "r"(input[8]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[1]) : "r"(input[9]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[2]) : "r"(input[10]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[3]) : "r"(input[11]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[4]) : "r"(input[12]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[5]) : "r"(input[13]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[6]) : "r"(input[14]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[7]) : "r"(input[15]));
    asm("st.v4.b32 [%4], {%0, %1, %2, %3};"
      : : "r"(input[0]), "r"(input[1]), "r"(input[2]), "r"(input[3]),
      __MEM_PTR(&B[8]));
    asm("st.v4.b32 [%4 + 16], {%0, %1, %2, %3};"
      : : "r"(input[4]), "r"(input[5]), "r"(input[6]), "r"(input[7]),
      __MEM_PTR(&B[8]));

    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4];"
      : "=r"(key[0]), "=r"(key[1]), "=r"(key[2]), "=r"(key[3])
      : __MEM_PTR(&(Tr + shiftTr)[2]));
    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4 + 16];"
      : "=r"(key[4]), "=r"(key[5]), "=r"(key[6]), "=r"(key[7])
      : __MEM_PTR(&(Tr + shiftTr)[2]));
    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4];"
      : "=r"(key[8]), "=r"(key[9]), "=r"(key[10]), "=r"(key[11])
      : __MEM_PTR(&(Tr2 + shiftTr)[2]));
    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4 + 16];"
      : "=r"(key[12]), "=r"(key[13]), "=r"(key[14]), "=r"(key[15])
      : __MEM_PTR(&(Tr2 + shiftTr)[2]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[0]) : "r"(key[8]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[1]) : "r"(key[9]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[2]) : "r"(key[10]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[3]) : "r"(key[11]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[4]) : "r"(key[12]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[5]) : "r"(key[13]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[6]) : "r"(key[14]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[7]) : "r"(key[15]));
    asm("st.v4.b32 [%4], {%0, %1, %2, %3};"
      : : "r"(key[0]), "r"(key[1]), "r"(key[2]), "r"(key[3]),
      __MEM_PTR(&B[16]));
    asm("st.v4.b32 [%4 + 16], {%0, %1, %2, %3};"
      : : "r"(key[4]), "r"(key[5]), "r"(key[6]), "r"(key[7]),
      __MEM_PTR(&B[16]));

    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4];"
      : "=r"(input[0]), "=r"(input[1]), "=r"(input[2]), "=r"(input[3])
      : __MEM_PTR(&(Tr + shiftTr)[3]));
    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4 + 16];"
      : "=r"(input[4]), "=r"(input[5]), "=r"(input[6]), "=r"(input[7])
      : __MEM_PTR(&(Tr + shiftTr)[3]));
    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4];"
      : "=r"(input[8]), "=r"(input[9]), "=r"(input[10]), "=r"(input[11])
      : __MEM_PTR(&(Tr2 + shiftTr)[3]));
    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4 + 16];"
      : "=r"(input[12]), "=r"(input[13]), "=r"(input[14]), "=r"(input[15])
      : __MEM_PTR(&(Tr2 + shiftTr)[3]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[0]) : "r"(input[8]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[1]) : "r"(input[9]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[2]) : "r"(input[10]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[3]) : "r"(input[11]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[4]) : "r"(input[12]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[5]) : "r"(input[13]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[6]) : "r"(input[14]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[7]) : "r"(input[15]));
    asm("st.v4.b32 [%4], {%0, %1, %2, %3};"
      : : "r"(input[0]), "r"(input[1]), "r"(input[2]), "r"(input[3]),
      __MEM_PTR(&B[24]));
    asm("st.v4.b32 [%4 + 16], {%0, %1, %2, %3};"
      : : "r"(input[4]), "r"(input[5]), "r"(input[6]), "r"(input[7]),
      __MEM_PTR(&B[24]));

    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4];"
      : "=r"(key[0]), "=r"(key[1]), "=r"(key[2]), "=r"(key[3])
      : __MEM_PTR(&(Tr + shiftTr)[4]));
    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4 + 16];"
      : "=r"(key[4]), "=r"(key[5]), "=r"(key[6]), "=r"(key[7])
      : __MEM_PTR(&(Tr + shiftTr)[4]));
    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4];"
      : "=r"(key[8]), "=r"(key[9]), "=r"(key[10]), "=r"(key[11])
      : __MEM_PTR(&(Tr2 + shiftTr)[4]));
    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4 + 16];"
      : "=r"(key[12]), "=r"(key[13]), "=r"(key[14]), "=r"(key[15])
      : __MEM_PTR(&(Tr2 + shiftTr)[4]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[0]) : "r"(key[8]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[1]) : "r"(key[9]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[2]) : "r"(key[10]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[3]) : "r"(key[11]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[4]) : "r"(key[12]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[5]) : "r"(key[13]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[6]) : "r"(key[14]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[7]) : "r"(key[15]));
    asm("st.v4.b32 [%4], {%0, %1, %2, %3};"
      : : "r"(key[0]), "r"(key[1]), "r"(key[2]), "r"(key[3]),
      __MEM_PTR(&B[32]));
    asm("st.v4.b32 [%4 + 16], {%0, %1, %2, %3};"
      : : "r"(key[4]), "r"(key[5]), "r"(key[6]), "r"(key[7]),
      __MEM_PTR(&B[32]));

    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4];"
      : "=r"(input[0]), "=r"(input[1]), "=r"(input[2]), "=r"(input[3])
      : __MEM_PTR(&(Tr + shiftTr)[5]));
    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4 + 16];"
      : "=r"(input[4]), "=r"(input[5]), "=r"(input[6]), "=r"(input[7])
      : __MEM_PTR(&(Tr + shiftTr)[5]));
    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4];"
      : "=r"(input[8]), "=r"(input[9]), "=r"(input[10]), "=r"(input[11])
      : __MEM_PTR(&(Tr2 + shiftTr)[5]));
    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4 + 16];"
      : "=r"(input[12]), "=r"(input[13]), "=r"(input[14]), "=r"(input[15])
      : __MEM_PTR(&(Tr2 + shiftTr)[5]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[0]) : "r"(input[8]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[1]) : "r"(input[9]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[2]) : "r"(input[10]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[3]) : "r"(input[11]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[4]) : "r"(input[12]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[5]) : "r"(input[13]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[6]) : "r"(input[14]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[7]) : "r"(input[15]));
    asm("st.v4.b32 [%4], {%0, %1, %2, %3};"
      : : "r"(input[0]), "r"(input[1]), "r"(input[2]), "r"(input[3]),
      __MEM_PTR(&B[40]));
    asm("st.v4.b32 [%4 + 16], {%0, %1, %2, %3};"
      : : "r"(input[4]), "r"(input[5]), "r"(input[6]), "r"(input[7]),
      __MEM_PTR(&B[40]));

    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4];"
      : "=r"(key[0]), "=r"(key[1]), "=r"(key[2]), "=r"(key[3])
      : __MEM_PTR(&(Tr + shiftTr)[6]));
    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4 + 16];"
      : "=r"(key[4]), "=r"(key[5]), "=r"(key[6]), "=r"(key[7])
      : __MEM_PTR(&(Tr + shiftTr)[6]));
    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4];"
      : "=r"(key[8]), "=r"(key[9]), "=r"(key[10]), "=r"(key[11])
      : __MEM_PTR(&(Tr2 + shiftTr)[6]));
    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4 + 16];"
      : "=r"(key[12]), "=r"(key[13]), "=r"(key[14]), "=r"(key[15])
      : __MEM_PTR(&(Tr2 + shiftTr)[6]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[0]) : "r"(key[8]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[1]) : "r"(key[9]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[2]) : "r"(key[10]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[3]) : "r"(key[11]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[4]) : "r"(key[12]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[5]) : "r"(key[13]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[6]) : "r"(key[14]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[7]) : "r"(key[15]));
    asm("st.v4.b32 [%4], {%0, %1, %2, %3};"
      : : "r"(key[0]), "r"(key[1]), "r"(key[2]), "r"(key[3]),
      __MEM_PTR(&B[48]));
    asm("st.v4.b32 [%4 + 16], {%0, %1, %2, %3};"
      : : "r"(key[4]), "r"(key[5]), "r"(key[6]), "r"(key[7]),
      __MEM_PTR(&B[48]));

    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4];"
      : "=r"(input[0]), "=r"(input[1]), "=r"(input[2]), "=r"(input[3])
      : __MEM_PTR(&(Tr + shiftTr)[7]));
    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4 + 16];"
      : "=r"(input[4]), "=r"(input[5]), "=r"(input[6]), "=r"(input[7])
      : __MEM_PTR(&(Tr + shiftTr)[7]));
    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4];"
      : "=r"(input[8]), "=r"(input[9]), "=r"(input[10]), "=r"(input[11])
      : __MEM_PTR(&(Tr2 + shiftTr)[7]));
    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4 + 16];"
      : "=r"(input[12]), "=r"(input[13]), "=r"(input[14]), "=r"(input[15])
      : __MEM_PTR(&(Tr2 + shiftTr)[7]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[0]) : "r"(input[8]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[1]) : "r"(input[9]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[2]) : "r"(input[10]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[3]) : "r"(input[11]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[4]) : "r"(input[12]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[5]) : "r"(input[13]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[6]) : "r"(input[14]));
    asm("xor.b32 %0, %0, %1;" : "+r"(input[7]) : "r"(input[15]));
    asm("st.v4.b32 [%4], {%0, %1, %2, %3};"
      : : "r"(input[0]), "r"(input[1]), "r"(input[2]), "r"(input[3]),
      __MEM_PTR(&B[56]));
    asm("st.v4.b32 [%4 + 16], {%0, %1, %2, %3};"
      : : "r"(input[4]), "r"(input[5]), "r"(input[6]), "r"(input[7]),
      __MEM_PTR(&B[56]));

    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4];"
      : "=r"(key[0]), "=r"(key[1]), "=r"(key[2]), "=r"(key[3])
      : __MEM_PTR(&(Tr + shiftTr)[0]));
    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4 + 16];"
      : "=r"(key[4]), "=r"(key[5]), "=r"(key[6]), "=r"(key[7])
      : __MEM_PTR(&(Tr + shiftTr)[0]));
    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4];"
      : "=r"(key[8]), "=r"(key[9]), "=r"(key[10]), "=r"(key[11])
      : __MEM_PTR(&(Tr2 + shiftTr)[0]));
    asm("ld.global.v4.b32 {%0, %1, %2, %3}, [%4 + 16];"
      : "=r"(key[12]), "=r"(key[13]), "=r"(key[14]), "=r"(key[15])
      : __MEM_PTR(&(Tr2 + shiftTr)[0]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[0]) : "r"(key[8]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[1]) : "r"(key[9]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[2]) : "r"(key[10]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[3]) : "r"(key[11]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[4]) : "r"(key[12]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[5]) : "r"(key[13]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[6]) : "r"(key[14]));
    asm("xor.b32 %0, %0, %1;" : "+r"(key[7]) : "r"(key[15]));
    asm("st.v4.b32 [%4], {%0, %1, %2, %3};"
      : : "r"(key[0]), "r"(key[1]), "r"(key[2]), "r"(key[3]),
      __MEM_PTR(&B[0]));
    asm("st.v4.b32 [%4 + 16], {%0, %1, %2, %3};"
      : : "r"(key[4]), "r"(key[5]), "r"(key[6]), "r"(key[7]),
      __MEM_PTR(&B[0]));

    asm("ldu.v4.b32 {%0, %1, %2, %3}, [%4];"
      : "=r"(input[0]), "=r"(input[1]), "=r"(input[2]), "=r"(input[3])
      : __MEM_PTR(&c_data[0]));
    asm("ldu.v4.b32 {%0, %1, %2, %3}, [%4];"
      : "=r"(input[4]), "=r"(input[5]), "=r"(input[6]), "=r"(input[7])
      : __MEM_PTR(&c_data[4]));
    asm("ldu.v4.b32 {%0, %1, %2, %3}, [%4];"
      : "=r"(input[8]), "=r"(input[9]), "=r"(input[10]), "=r"(input[11])
      : __MEM_PTR(&c_data[8]));
    asm("ldu.v4.b32 {%0, %1, %2, %3}, [%4];"
      : "=r"(input[12]), "=r"(input[13]), "=r"(input[14]), "=r"(input[15])
      : __MEM_PTR(&c_data[12]));

    key[8] = 0;  key[9] = 0;  key[10] = 0; key[11] = 0;
    key[12] = 0; key[13] = 0; key[14] = 0; key[15] = 0;

    uint temp[9];
    uint bufptr, qbuf, bitbuf, shift, a, b;
    const uint c63 = 0x3F;

    for(i = 0; i < 32; i++) {

        blake2s(input, input, key);

        bufptr = input[0] + input[1] + input[2] + input[3] +
                 input[4] + input[5] + input[6] + input[7];
        bufptr += (input[0] >> 8)  + (input[1] >> 8) +
                  (input[2] >> 8)  + (input[3] >> 8) +
                  (input[4] >> 8)  + (input[5] >> 8) +
                  (input[6] >> 8)  + (input[7] >> 8);
        bufptr += (input[0] >> 16) + (input[1] >> 16) +
                  (input[2] >> 16) + (input[3] >> 16) +
                  (input[4] >> 16) + (input[5] >> 16) +
                  (input[6] >> 16) + (input[7] >> 16);
        bufptr += (input[0] >> 24) + (input[1] >> 24) +
                  (input[2] >> 24) + (input[3] >> 24) +
                  (input[4] >> 24) + (input[5] >> 24) +
                  (input[6] >> 24) + (input[7] >> 24);
        bufptr &= 0xFF;

        qbuf = bufptr >> 2;
        bitbuf = (bufptr & 0x03) << 3;
 
        if(i == 31) continue;

        shift = 32 - bitbuf;

    asm("ld.b32 %0, [%1];" : "=r"(temp[0]) : __MEM_PTR(&B[qbuf & c63]));
    asm("ld.b32 %0, [%1];" : "=r"(temp[1]) : __MEM_PTR(&B[(qbuf + 1) & c63]));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(a) : "r"(0), "r"(input[0]), "r"(shift));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(b) : "r"(input[0]), "r"(input[1]), "r"(shift));
    asm("xor.b32 %0, %0, %1;" : "+r"(temp[0]) : "r"(a));
    asm("xor.b32 %0, %0, %1;" : "+r"(temp[1]) : "r"(b));
    asm("st.b32 [%1], %0;" : : "r"(temp[0]), __MEM_PTR(&B[qbuf & c63]));

    asm("ld.b32 %0, [%1];" : "=r"(temp[2]) : __MEM_PTR(&B[(qbuf + 2) & c63]));
    asm("ld.b32 %0, [%1];" : "=r"(temp[3]) : __MEM_PTR(&B[(qbuf + 3) & c63]));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(a) : "r"(input[1]), "r"(input[2]), "r"(shift));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(b) : "r"(input[2]), "r"(input[3]), "r"(shift));
    asm("xor.b32 %0, %0, %1;" : "+r"(temp[2]) : "r"(a));
    asm("xor.b32 %0, %0, %1;" : "+r"(temp[3]) : "r"(b));
    asm("st.b32 [%1], %0;" : : "r"(temp[1]), __MEM_PTR(&B[(qbuf + 1) & c63]));

    asm("ld.b32 %0, [%1];" : "=r"(temp[4]) : __MEM_PTR(&B[(qbuf + 4) & c63]));
    asm("ld.b32 %0, [%1];" : "=r"(temp[5]) : __MEM_PTR(&B[(qbuf + 5) & c63]));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(a) : "r"(input[3]), "r"(input[4]), "r"(shift));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(b) : "r"(input[4]), "r"(input[5]), "r"(shift));
    asm("xor.b32 %0, %0, %1;" : "+r"(temp[4]) : "r"(a));
    asm("xor.b32 %0, %0, %1;" : "+r"(temp[5]) : "r"(b));
    asm("st.b32 [%1], %0;" : : "r"(temp[2]), __MEM_PTR(&B[(qbuf + 2) & c63]));

    asm("ld.b32 %0, [%1];" : "=r"(temp[6]) : __MEM_PTR(&B[(qbuf + 6) & c63]));
    asm("ld.b32 %0, [%1];" : "=r"(temp[7]) : __MEM_PTR(&B[(qbuf + 7) & c63]));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(a) : "r"(input[5]), "r"(input[6]), "r"(shift));
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(b) : "r"(input[6]), "r"(input[7]), "r"(shift));
    asm("xor.b32 %0, %0, %1;" : "+r"(temp[6]) : "r"(a));
    asm("xor.b32 %0, %0, %1;" : "+r"(temp[7]) : "r"(b));
    asm("st.b32 [%1], %0;" : : "r"(temp[3]), __MEM_PTR(&B[(qbuf + 3) & c63]));

    asm("ld.b32 %0, [%1];" : "=r"(temp[8]) : __MEM_PTR(&B[(qbuf + 8) & c63]));
    asm("shr.b32 %0, %1, %2;" : "=r"(a) : "r"(input[7]), "r"(shift));
    asm("xor.b32 %0, %0, %1;" : "+r"(temp[8]) : "r"(a));
    asm("st.b32 [%1], %0;" : : "r"(temp[4]), __MEM_PTR(&B[(qbuf + 4) & c63]));
    asm("st.b32 [%1], %0;" : : "r"(temp[5]), __MEM_PTR(&B[(qbuf + 5) & c63]));
    asm("st.b32 [%1], %0;" : : "r"(temp[6]), __MEM_PTR(&B[(qbuf + 6) & c63]));
    asm("st.b32 [%1], %0;" : : "r"(temp[7]), __MEM_PTR(&B[(qbuf + 7) & c63]));
    asm("st.b32 [%1], %0;" : : "r"(temp[8]), __MEM_PTR(&B[(qbuf + 8) & c63]));

        a = c_data[qbuf & c63];
        for(j = 0; j < 16; j += 2) {
            b = c_data[(qbuf + j + 1) & c63];
        asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(input[j]) : "r"(a), "r"(b), "r"(bitbuf));
            a = c_data[(qbuf + j + 2) & c63];
        asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(input[j + 1]) : "r"(b), "r"(a), "r"(bitbuf));

    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(key[j >> 1])
      : "r"(temp[j >> 1]), "r"(temp[(j >> 1) + 1]), "r"(bitbuf));
        }

        if(qbuf < 60) {
            uint noncepos = 19 - qbuf % 20;
            if(noncepos <= 16) {
                if(noncepos != 0)
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(input[noncepos - 1])
      : "r"(c_data[18]), "r"(nonce), "r"(bitbuf));
                if(noncepos !=16)
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(input[noncepos])
      : "r"(nonce), "r"(c_data[20]), "r"(bitbuf));
            }
        }

    }

    a = B[(qbuf + 7) & c63];
    b = B[(qbuf + 8) & c63];
    asm("shf.r.clamp.b32 %0, %1, %2, %3;" : "=r"(i) : "r"(a), "r"(b), "r"(bitbuf));
    asm("xor.b32 %0, %0, %1;" : "+r"(i) : "r"(input[7]));
    asm("xor.b32 %0, %0, %1;" : "+r"(i) : "r"(data7));

    if(i <= hash_target)
    asm("st.b32 [%1], %0;" : : "r"(nonce), __MEM_PTR(&nonceVector[0]));
}


static __device__ __forceinline__
void salsa_core_mode1(uint *Y) {
    const uint c7 = 7, c9 = 9, c13 = 13, c18 = 18;

    asm("{\n"
      ".reg .u32 t;\n"
      "add.u32 t, %0, %3;\n"
      "shf.l.wrap.b32 t, t, t, %4;\n"
      "xor.b32 %1, %1, t;\n"
      "add.u32 t, %1, %0;\n"
      "shf.l.wrap.b32 t, t, t, %5;\n"
      "xor.b32 %2, %2, t;\n"
      "add.u32 t, %2, %1;\n"
      "shf.l.wrap.b32 t, t, t, %6;\n"
      "xor.b32 %3, %3, t;\n"
      "add.u32 t, %3, %2;\n"
      "shf.l.wrap.b32 t, t, t, %7;\n"
      "xor.b32 %0, %0, t;\n"
    "}" : "+r"(Y[0]), "+r"(Y[4]), "+r"(Y[8]), "+r"(Y[12])
        : "r"(c7), "r"(c9), "r"(c13), "r"(c18));

    asm("{\n"
      ".reg .u32 t;\n"
      "add.u32 t, %0, %3;\n"
      "shf.l.wrap.b32 t, t, t, %4;\n"
      "xor.b32 %1, %1, t;\n"
      "add.u32 t, %1, %0;\n"
      "shf.l.wrap.b32 t, t, t, %5;\n"
      "xor.b32 %2, %2, t;\n"
      "add.u32 t, %2, %1;\n"
      "shf.l.wrap.b32 t, t, t, %6;\n"
      "xor.b32 %3, %3, t;\n"
      "add.u32 t, %3, %2;\n"
      "shf.l.wrap.b32 t, t, t, %7;\n"
      "xor.b32 %0, %0, t;\n"
    "}" : "+r"(Y[5]), "+r"(Y[9]), "+r"(Y[13]), "+r"(Y[1])
        : "r"(c7), "r"(c9), "r"(c13), "r"(c18));

    asm("{\n"
      ".reg .u32 t;\n"
      "add.u32 t, %0, %3;\n"
      "shf.l.wrap.b32 t, t, t, %4;\n"
      "xor.b32 %1, %1, t;\n"
      "add.u32 t, %1, %0;\n"
      "shf.l.wrap.b32 t, t, t, %5;\n"
      "xor.b32 %2, %2, t;\n"
      "add.u32 t, %2, %1;\n"
      "shf.l.wrap.b32 t, t, t, %6;\n"
      "xor.b32 %3, %3, t;\n"
      "add.u32 t, %3, %2;\n"
      "shf.l.wrap.b32 t, t, t, %7;\n"
      "xor.b32 %0, %0, t;\n"
    "}" : "+r"(Y[10]), "+r"(Y[14]), "+r"(Y[2]), "+r"(Y[6])
        : "r"(c7), "r"(c9), "r"(c13), "r"(c18));

    asm("{\n"
      ".reg .u32 t;\n"
      "add.u32 t, %0, %3;\n"
      "shf.l.wrap.b32 t, t, t, %4;\n"
      "xor.b32 %1, %1, t;\n"
      "add.u32 t, %1, %0;\n"
      "shf.l.wrap.b32 t, t, t, %5;\n"
      "xor.b32 %2, %2, t;\n"
      "add.u32 t, %2, %1;\n"
      "shf.l.wrap.b32 t, t, t, %6;\n"
      "xor.b32 %3, %3, t;\n"
      "add.u32 t, %3, %2;\n"
      "shf.l.wrap.b32 t, t, t, %7;\n"
      "xor.b32 %0, %0, t;\n"
    "}" : "+r"(Y[15]), "+r"(Y[3]), "+r"(Y[7]), "+r"(Y[11])
        : "r"(c7), "r"(c9), "r"(c13), "r"(c18));

    asm("{\n"
      ".reg .u32 t;\n"
      "add.u32 t, %0, %3;\n"
      "shf.l.wrap.b32 t, t, t, %4;\n"
      "xor.b32 %1, %1, t;\n"
      "add.u32 t, %1, %0;\n"
      "shf.l.wrap.b32 t, t, t, %5;\n"
      "xor.b32 %2, %2, t;\n"
      "add.u32 t, %2, %1;\n"
      "shf.l.wrap.b32 t, t, t, %6;\n"
      "xor.b32 %3, %3, t;\n"
      "add.u32 t, %3, %2;\n"
      "shf.l.wrap.b32 t, t, t, %7;\n"
      "xor.b32 %0, %0, t;\n"
    "}" : "+r"(Y[0]), "+r"(Y[1]), "+r"(Y[2]), "+r"(Y[3])
        : "r"(c7), "r"(c9), "r"(c13), "r"(c18));

    asm("{\n"
      ".reg .u32 t;\n"
      "add.u32 t, %0, %3;\n"
      "shf.l.wrap.b32 t, t, t, %4;\n"
      "xor.b32 %1, %1, t;\n"
      "add.u32 t, %1, %0;\n"
      "shf.l.wrap.b32 t, t, t, %5;\n"
      "xor.b32 %2, %2, t;\n"
      "add.u32 t, %2, %1;\n"
      "shf.l.wrap.b32 t, t, t, %6;\n"
      "xor.b32 %3, %3, t;\n"
      "add.u32 t, %3, %2;\n"
      "shf.l.wrap.b32 t, t, t, %7;\n"
      "xor.b32 %0, %0, t;\n"
    "}" : "+r"(Y[5]), "+r"(Y[6]), "+r"(Y[7]), "+r"(Y[4])
        : "r"(c7), "r"(c9), "r"(c13), "r"(c18));

    asm("{\n"
      ".reg .u32 t;\n"
      "add.u32 t, %0, %3;\n"
      "shf.l.wrap.b32 t, t, t, %4;\n"
      "xor.b32 %1, %1, t;\n"
      "add.u32 t, %1, %0;\n"
      "shf.l.wrap.b32 t, t, t, %5;\n"
      "xor.b32 %2, %2, t;\n"
      "add.u32 t, %2, %1;\n"
      "shf.l.wrap.b32 t, t, t, %6;\n"
      "xor.b32 %3, %3, t;\n"
      "add.u32 t, %3, %2;\n"
      "shf.l.wrap.b32 t, t, t, %7;\n"
      "xor.b32 %0, %0, t;\n"
    "}" : "+r"(Y[10]), "+r"(Y[11]), "+r"(Y[8]), "+r"(Y[9])
        : "r"(c7), "r"(c9), "r"(c13), "r"(c18));

    asm("{\n"
      ".reg .u32 t;\n"
      "add.u32 t, %0, %3;\n"
      "shf.l.wrap.b32 t, t, t, %4;\n"
      "xor.b32 %1, %1, t;\n"
      "add.u32 t, %1, %0;\n"
      "shf.l.wrap.b32 t, t, t, %5;\n"
      "xor.b32 %2, %2, t;\n"
      "add.u32 t, %2, %1;\n"
      "shf.l.wrap.b32 t, t, t, %6;\n"
      "xor.b32 %3, %3, t;\n"
      "add.u32 t, %3, %2;\n"
      "shf.l.wrap.b32 t, t, t, %7;\n"
      "xor.b32 %0, %0, t;\n"
    "}" : "+r"(Y[15]), "+r"(Y[12]), "+r"(Y[13]), "+r"(Y[14])
        : "r"(c7), "r"(c9), "r"(c13), "r"(c18));
}

static __device__ __forceinline__
void chacha_core_mode1(uint *Y) {
    const uint c7 = 7, c8 = 0x2103, c12 = 12, c16 = 0x1032, c0 = 0;

    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %4;\n"
    "}" : "+r"(Y[0]), "+r"(Y[4]), "+r"(Y[8]), "+r"(Y[12])
        : "r"(c7), "r"(c8), "r"(c12), "r"(c16), "r"(c0));

    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %4;\n"
    "}" : "+r"(Y[1]), "+r"(Y[5]), "+r"(Y[9]), "+r"(Y[13])
        : "r"(c7), "r"(c8), "r"(c12), "r"(c16), "r"(c0));

    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %4;\n"
    "}" : "+r"(Y[2]), "+r"(Y[6]), "+r"(Y[10]), "+r"(Y[14])
        : "r"(c7), "r"(c8), "r"(c12), "r"(c16), "r"(c0));

    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %4;\n"
    "}" : "+r"(Y[3]), "+r"(Y[7]), "+r"(Y[11]), "+r"(Y[15])
        : "r"(c7), "r"(c8), "r"(c12), "r"(c16), "r"(c0));

    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %4;\n"
    "}" : "+r"(Y[0]), "+r"(Y[5]), "+r"(Y[10]), "+r"(Y[15])
        : "r"(c7), "r"(c8), "r"(c12), "r"(c16), "r"(c0));

    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %4;\n"
    "}" : "+r"(Y[1]), "+r"(Y[6]), "+r"(Y[11]), "+r"(Y[12])
        : "r"(c7), "r"(c8), "r"(c12), "r"(c16), "r"(c0));

    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %4;\n"
    "}" : "+r"(Y[2]), "+r"(Y[7]), "+r"(Y[8]), "+r"(Y[13])
        : "r"(c7), "r"(c8), "r"(c12), "r"(c16), "r"(c0));

    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %4;\n"
    "}" : "+r"(Y[3]), "+r"(Y[4]), "+r"(Y[9]), "+r"(Y[14])
        : "r"(c7), "r"(c8), "r"(c12), "r"(c16), "r"(c0));
}

static __device__ __forceinline__
void neoscrypt_salsa_mode1(uint *XV) {
    uint Y[16];
    uint i;

    ((ulong8 *) Y)[0] = ((ulong8 *) XV)[0];

    Y[0]  ^= XV[48]; Y[1]  ^= XV[49]; Y[2]  ^= XV[50]; Y[3]  ^= XV[51];
    Y[4]  ^= XV[52]; Y[5]  ^= XV[53]; Y[6]  ^= XV[54]; Y[7]  ^= XV[55];
    Y[8]  ^= XV[56]; Y[9]  ^= XV[57]; Y[10] ^= XV[58]; Y[11] ^= XV[59];
    Y[12] ^= XV[60]; Y[13] ^= XV[61]; Y[14] ^= XV[62]; Y[15] ^= XV[63];

    ((ulong8 *) XV)[0] = ((ulong8 *) Y)[0];

    for(i = 0; i < 10; i++)
      salsa_core_mode1(Y);

    Y[0]  += XV[0];  Y[1]  += XV[1];  Y[2]  += XV[2];  Y[3]  += XV[3];
    Y[4]  += XV[4];  Y[5]  += XV[5];  Y[6]  += XV[6];  Y[7]  += XV[7];
    Y[8]  += XV[8];  Y[9]  += XV[9];  Y[10] += XV[10]; Y[11] += XV[11];
    Y[12] += XV[12]; Y[13] += XV[13]; Y[14] += XV[14]; Y[15] += XV[15];

    ((ulong8 *) XV)[0] = ((ulong8 *) Y)[0];

    Y[0]  ^= XV[16]; Y[1]  ^= XV[17]; Y[2]  ^= XV[18]; Y[3]  ^= XV[19];
    Y[4]  ^= XV[20]; Y[5]  ^= XV[21]; Y[6]  ^= XV[22]; Y[7]  ^= XV[23];
    Y[8]  ^= XV[24]; Y[9]  ^= XV[25]; Y[10] ^= XV[26]; Y[11] ^= XV[27];
    Y[12] ^= XV[28]; Y[13] ^= XV[29]; Y[14] ^= XV[30]; Y[15] ^= XV[31];

    ((ulong8 *) XV)[1] = ((ulong8 *) Y)[0];

    for(i = 0; i < 10; i++)
      salsa_core_mode1(Y);

    Y[0]  += XV[16];  Y[1]  += XV[17];  Y[2]  += XV[18];  Y[3] += XV[19];
    Y[4]  += XV[20];  Y[5]  += XV[21];  Y[6]  += XV[22];  Y[7] += XV[23];
    Y[8]  += XV[24];  Y[9]  += XV[25];  Y[10] += XV[26]; Y[11] += XV[27];
    Y[12] += XV[28];  Y[13] += XV[29];  Y[14] += XV[30]; Y[15] += XV[31];

    ((ulong8 *) XV)[1] = ((ulong8 *) Y)[0];

    Y[0]  ^= XV[32]; Y[1]  ^= XV[33]; Y[2]  ^= XV[34]; Y[3]  ^= XV[35];
    Y[4]  ^= XV[36]; Y[5]  ^= XV[37]; Y[6]  ^= XV[38]; Y[7]  ^= XV[39];
    Y[8]  ^= XV[40]; Y[9]  ^= XV[41]; Y[10] ^= XV[42]; Y[11] ^= XV[43];
    Y[12] ^= XV[44]; Y[13] ^= XV[45]; Y[14] ^= XV[46]; Y[15] ^= XV[47];

    ((ulong8 *) XV)[2] = ((ulong8 *) Y)[0];

    for(i = 0; i < 10; i++)
      salsa_core_mode1(Y);

    Y[0]  += XV[32];  Y[1]  += XV[33];  Y[2]  += XV[34];  Y[3] += XV[35];
    Y[4]  += XV[36];  Y[5]  += XV[37];  Y[6]  += XV[38];  Y[7] += XV[39];
    Y[8]  += XV[40];  Y[9]  += XV[41];  Y[10] += XV[42]; Y[11] += XV[43];
    Y[12] += XV[44];  Y[13] += XV[45];  Y[14] += XV[46]; Y[15] += XV[47];

    ((ulong8 *) XV)[2] = ((ulong8 *) Y)[0];

    Y[0]  ^= XV[48]; Y[1]  ^= XV[49]; Y[2]  ^= XV[50]; Y[3]  ^= XV[51];
    Y[4]  ^= XV[52]; Y[5]  ^= XV[53]; Y[6]  ^= XV[54]; Y[7]  ^= XV[55];
    Y[8]  ^= XV[56]; Y[9]  ^= XV[57]; Y[10] ^= XV[58]; Y[11] ^= XV[59];
    Y[12] ^= XV[60]; Y[13] ^= XV[61]; Y[14] ^= XV[62]; Y[15] ^= XV[63];

    ((ulong8 *) XV)[3] = ((ulong8 *) Y)[0];

    for(i = 0; i < 10; i++)
      salsa_core_mode1(Y);

    Y[0]  += XV[48];  Y[1]  += XV[49];  Y[2]  += XV[50];  Y[3] += XV[51];
    Y[4]  += XV[52];  Y[5]  += XV[53];  Y[6]  += XV[54];  Y[7] += XV[55];
    Y[8]  += XV[56];  Y[9]  += XV[57];  Y[10] += XV[58]; Y[11] += XV[59];
    Y[12] += XV[60];  Y[13] += XV[61];  Y[14] += XV[62]; Y[15] += XV[63];

    ((ulong8 *) XV)[3] = ((ulong8 *) Y)[0];

    XV[32] ^= XV[16]; XV[33] ^= XV[17]; XV[34] ^= XV[18]; XV[35] ^= XV[19];
    XV[36] ^= XV[20]; XV[37] ^= XV[21]; XV[38] ^= XV[22]; XV[39] ^= XV[23];
    XV[40] ^= XV[24]; XV[41] ^= XV[25]; XV[42] ^= XV[26]; XV[43] ^= XV[27];
    XV[44] ^= XV[28]; XV[45] ^= XV[29]; XV[46] ^= XV[30]; XV[47] ^= XV[31];

    XV[16] ^= XV[32]; XV[17] ^= XV[33]; XV[18] ^= XV[34]; XV[19] ^= XV[35];
    XV[20] ^= XV[36]; XV[21] ^= XV[37]; XV[22] ^= XV[38]; XV[23] ^= XV[39];
    XV[24] ^= XV[40]; XV[25] ^= XV[41]; XV[26] ^= XV[42]; XV[27] ^= XV[43];
    XV[28] ^= XV[44]; XV[29] ^= XV[45]; XV[30] ^= XV[46]; XV[31] ^= XV[47];

    XV[32] ^= XV[16]; XV[33] ^= XV[17]; XV[34] ^= XV[18]; XV[35] ^= XV[19];
    XV[36] ^= XV[20]; XV[37] ^= XV[21]; XV[38] ^= XV[22]; XV[39] ^= XV[23];
    XV[40] ^= XV[24]; XV[41] ^= XV[25]; XV[42] ^= XV[26]; XV[43] ^= XV[27];
    XV[44] ^= XV[28]; XV[45] ^= XV[29]; XV[46] ^= XV[30]; XV[47] ^= XV[31];
}

static __device__ __forceinline__
void neoscrypt_chacha_mode1(uint *XV) {
    uint Y[16];
    uint i;

    ((ulong8 *) Y)[0] = ((ulong8 *) XV)[0];

    Y[0]  ^= XV[48]; Y[1]  ^= XV[49]; Y[2]  ^= XV[50]; Y[3]  ^= XV[51];
    Y[4]  ^= XV[52]; Y[5]  ^= XV[53]; Y[6]  ^= XV[54]; Y[7]  ^= XV[55];
    Y[8]  ^= XV[56]; Y[9]  ^= XV[57]; Y[10] ^= XV[58]; Y[11] ^= XV[59];
    Y[12] ^= XV[60]; Y[13] ^= XV[61]; Y[14] ^= XV[62]; Y[15] ^= XV[63];

    ((ulong8 *) XV)[0] = ((ulong8 *) Y)[0];

    for(i = 0; i < 10; i++)
      chacha_core_mode1(Y);

    Y[0]  += XV[0];  Y[1]  += XV[1];  Y[2]  += XV[2];  Y[3]  += XV[3];
    Y[4]  += XV[4];  Y[5]  += XV[5];  Y[6]  += XV[6];  Y[7]  += XV[7];
    Y[8]  += XV[8];  Y[9]  += XV[9];  Y[10] += XV[10]; Y[11] += XV[11];
    Y[12] += XV[12]; Y[13] += XV[13]; Y[14] += XV[14]; Y[15] += XV[15];

    ((ulong8 *) XV)[0] = ((ulong8 *) Y)[0];

    Y[0]  ^= XV[16]; Y[1]  ^= XV[17]; Y[2]  ^= XV[18]; Y[3]  ^= XV[19];
    Y[4]  ^= XV[20]; Y[5]  ^= XV[21]; Y[6]  ^= XV[22]; Y[7]  ^= XV[23];
    Y[8]  ^= XV[24]; Y[9]  ^= XV[25]; Y[10] ^= XV[26]; Y[11] ^= XV[27];
    Y[12] ^= XV[28]; Y[13] ^= XV[29]; Y[14] ^= XV[30]; Y[15] ^= XV[31];

    ((ulong8 *) XV)[1] = ((ulong8 *) Y)[0];

    for(i = 0; i < 10; i++)
      chacha_core_mode1(Y);

    Y[0]  += XV[16];  Y[1]  += XV[17];  Y[2]  += XV[18];  Y[3] += XV[19];
    Y[4]  += XV[20];  Y[5]  += XV[21];  Y[6]  += XV[22];  Y[7] += XV[23];
    Y[8]  += XV[24];  Y[9]  += XV[25];  Y[10] += XV[26]; Y[11] += XV[27];
    Y[12] += XV[28];  Y[13] += XV[29];  Y[14] += XV[30]; Y[15] += XV[31];

    ((ulong8 *) XV)[1] = ((ulong8 *) Y)[0];

    Y[0]  ^= XV[32]; Y[1]  ^= XV[33]; Y[2]  ^= XV[34]; Y[3]  ^= XV[35];
    Y[4]  ^= XV[36]; Y[5]  ^= XV[37]; Y[6]  ^= XV[38]; Y[7]  ^= XV[39];
    Y[8]  ^= XV[40]; Y[9]  ^= XV[41]; Y[10] ^= XV[42]; Y[11] ^= XV[43];
    Y[12] ^= XV[44]; Y[13] ^= XV[45]; Y[14] ^= XV[46]; Y[15] ^= XV[47];

    ((ulong8 *) XV)[2] = ((ulong8 *) Y)[0];

    for(i = 0; i < 10; i++)
      chacha_core_mode1(Y);

    Y[0]  += XV[32];  Y[1]  += XV[33];  Y[2]  += XV[34];  Y[3] += XV[35];
    Y[4]  += XV[36];  Y[5]  += XV[37];  Y[6]  += XV[38];  Y[7] += XV[39];
    Y[8]  += XV[40];  Y[9]  += XV[41];  Y[10] += XV[42]; Y[11] += XV[43];
    Y[12] += XV[44];  Y[13] += XV[45];  Y[14] += XV[46]; Y[15] += XV[47];

    ((ulong8 *) XV)[2] = ((ulong8 *) Y)[0];

    Y[0]  ^= XV[48]; Y[1]  ^= XV[49]; Y[2]  ^= XV[50]; Y[3]  ^= XV[51];
    Y[4]  ^= XV[52]; Y[5]  ^= XV[53]; Y[6]  ^= XV[54]; Y[7]  ^= XV[55];
    Y[8]  ^= XV[56]; Y[9]  ^= XV[57]; Y[10] ^= XV[58]; Y[11] ^= XV[59];
    Y[12] ^= XV[60]; Y[13] ^= XV[61]; Y[14] ^= XV[62]; Y[15] ^= XV[63];

    ((ulong8 *) XV)[3] = ((ulong8 *) Y)[0];

    for(i = 0; i < 10; i++)
      chacha_core_mode1(Y);

    Y[0]  += XV[48];  Y[1]  += XV[49];  Y[2]  += XV[50];  Y[3] += XV[51];
    Y[4]  += XV[52];  Y[5]  += XV[53];  Y[6]  += XV[54];  Y[7] += XV[55];
    Y[8]  += XV[56];  Y[9]  += XV[57];  Y[10] += XV[58]; Y[11] += XV[59];
    Y[12] += XV[60];  Y[13] += XV[61];  Y[14] += XV[62]; Y[15] += XV[63];

    ((ulong8 *) XV)[3] = ((ulong8 *) Y)[0];

    XV[32] ^= XV[16]; XV[33] ^= XV[17]; XV[34] ^= XV[18]; XV[35] ^= XV[19];
    XV[36] ^= XV[20]; XV[37] ^= XV[21]; XV[38] ^= XV[22]; XV[39] ^= XV[23];
    XV[40] ^= XV[24]; XV[41] ^= XV[25]; XV[42] ^= XV[26]; XV[43] ^= XV[27];
    XV[44] ^= XV[28]; XV[45] ^= XV[29]; XV[46] ^= XV[30]; XV[47] ^= XV[31];

    XV[16] ^= XV[32]; XV[17] ^= XV[33]; XV[18] ^= XV[34]; XV[19] ^= XV[35];
    XV[20] ^= XV[36]; XV[21] ^= XV[37]; XV[22] ^= XV[38]; XV[23] ^= XV[39];
    XV[24] ^= XV[40]; XV[25] ^= XV[41]; XV[26] ^= XV[42]; XV[27] ^= XV[43];
    XV[28] ^= XV[44]; XV[29] ^= XV[45]; XV[30] ^= XV[46]; XV[31] ^= XV[47];

    XV[32] ^= XV[16]; XV[33] ^= XV[17]; XV[34] ^= XV[18]; XV[35] ^= XV[19];
    XV[36] ^= XV[20]; XV[37] ^= XV[21]; XV[38] ^= XV[22]; XV[39] ^= XV[23];
    XV[40] ^= XV[24]; XV[41] ^= XV[25]; XV[42] ^= XV[26]; XV[43] ^= XV[27];
    XV[44] ^= XV[28]; XV[45] ^= XV[29]; XV[46] ^= XV[30]; XV[47] ^= XV[31];
}


static __device__ __forceinline__
void salsa_core_mode3(uint *Y) {
    const uint c7 = 7, c9 = 9, c13 = 13, c18 = 18, exch = 0x1C1F;

    asm("{\n"
      ".reg .u32 t;\n"
      "add.u32 t, %0, %3;\n"
      "shf.l.wrap.b32 t, t, t, %4;\n"
      "xor.b32 %1, %1, t;\n"
      "add.u32 t, %1, %0;\n"
      "shf.l.wrap.b32 t, t, t, %5;\n"
      "xor.b32 %2, %2, t;\n"
      "add.u32 t, %2, %1;\n"
      "shf.l.wrap.b32 t, t, t, %6;\n"
      "xor.b32 %3, %3, t;\n"
      "add.u32 t, %3, %2;\n"
      "shf.l.wrap.b32 t, t, t, %7;\n"
      "xor.b32 %0, %0, t;\n"
    "}" : "+r"(Y[0]), "+r"(Y[1]), "+r"(Y[2]), "+r"(Y[3])
        : "r"(c7), "r"(c9), "r"(c13), "r"(c18));

   asm("shfl.idx.b32 %0, %0, %1, %2;" : "+r"(Y[1]) : "r"(threadIdx.x + 3), "r"(exch));
   asm("shfl.idx.b32 %0, %0, %1, %2;" : "+r"(Y[2]) : "r"(threadIdx.x + 2), "r"(exch));
   asm("shfl.idx.b32 %0, %0, %1, %2;" : "+r"(Y[3]) : "r"(threadIdx.x + 1), "r"(exch));

    asm("{\n"
      ".reg .u32 t;\n"
      "add.u32 t, %0, %3;\n"
      "shf.l.wrap.b32 t, t, t, %4;\n"
      "xor.b32 %1, %1, t;\n"
      "add.u32 t, %1, %0;\n"
      "shf.l.wrap.b32 t, t, t, %5;\n"
      "xor.b32 %2, %2, t;\n"
      "add.u32 t, %2, %1;\n"
      "shf.l.wrap.b32 t, t, t, %6;\n"
      "xor.b32 %3, %3, t;\n"
      "add.u32 t, %3, %2;\n"
      "shf.l.wrap.b32 t, t, t, %7;\n"
      "xor.b32 %0, %0, t;\n"
    "}" : "+r"(Y[0]), "+r"(Y[3]), "+r"(Y[2]), "+r"(Y[1])
        : "r"(c7), "r"(c9), "r"(c13), "r"(c18));

   asm("shfl.idx.b32 %0, %0, %1, %2;" : "+r"(Y[1]) : "r"(threadIdx.x + 1), "r"(exch));
   asm("shfl.idx.b32 %0, %0, %1, %2;" : "+r"(Y[2]) : "r"(threadIdx.x + 2), "r"(exch));
   asm("shfl.idx.b32 %0, %0, %1, %2;" : "+r"(Y[3]) : "r"(threadIdx.x + 3), "r"(exch));
}

static __device__ __forceinline__
void chacha_core_mode3(uint *Y) {
    const uint c7 = 7, c8 = 0x2103, c12 = 12, c16 = 0x1032, c0 = 0, exch = 0x1C1F;

    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %4;\n"
    "}" : "+r"(Y[0]), "+r"(Y[1]), "+r"(Y[2]), "+r"(Y[3])
        : "r"(c7), "r"(c8), "r"(c12), "r"(c16), "r"(c0));

   asm("shfl.idx.b32 %0, %0, %1, %2;" : "+r"(Y[1]) : "r"(threadIdx.x + 1), "r"(exch));
   asm("shfl.idx.b32 %0, %0, %1, %2;" : "+r"(Y[2]) : "r"(threadIdx.x + 2), "r"(exch));
   asm("shfl.idx.b32 %0, %0, %1, %2;" : "+r"(Y[3]) : "r"(threadIdx.x + 3), "r"(exch));

    asm("{\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %7;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %6;\n"
      "add.u32 %0, %0, %1;\n"
      "xor.b32 %3, %3, %0;\n"
      "prmt.b32 %3, %3, %8, %5;\n"
      "add.u32 %2, %2, %3;\n"
      "xor.b32 %1, %1, %2;\n"
      "shf.l.wrap.b32 %1, %1, %1, %4;\n"
    "}" : "+r"(Y[0]), "+r"(Y[1]), "+r"(Y[2]), "+r"(Y[3])
        : "r"(c7), "r"(c8), "r"(c12), "r"(c16), "r"(c0));

   asm("shfl.idx.b32 %0, %0, %1, %2;" : "+r"(Y[1]) : "r"(threadIdx.x + 3), "r"(exch));
   asm("shfl.idx.b32 %0, %0, %1, %2;" : "+r"(Y[2]) : "r"(threadIdx.x + 2), "r"(exch));
   asm("shfl.idx.b32 %0, %0, %1, %2;" : "+r"(Y[3]) : "r"(threadIdx.x + 1), "r"(exch));
}


static __device__ __forceinline__
void neoscrypt_salsa_mode3(uint4 *XV) {
    uint4 Y;
    uint i;

    Y = XV[0];

    Y ^= XV[3];

    XV[0] = Y;

    for(i = 0; i < 10; i++)
      salsa_core_mode3((uint *) &Y);

    Y += XV[0];

    XV[0] = Y;

    Y ^= XV[1];

    XV[1] = Y;

    for(i = 0; i < 10; i++)
      salsa_core_mode3((uint *) &Y);

    Y += XV[1];

    XV[1] = Y;

    Y ^= XV[2];

    XV[2] = Y;

    for(i = 0; i < 10; i++)
      salsa_core_mode3((uint *) &Y);

    Y += XV[2];

    XV[2] = Y;

    Y ^= XV[3];

    XV[3] = Y;

    for(i = 0; i < 10; i++)
      salsa_core_mode3((uint *) &Y);

    Y += XV[3];

    XV[3] = Y;

    XV[2] ^= XV[1];
    XV[1] ^= XV[2];
    XV[2] ^= XV[1];
}

static __device__ __forceinline__
void neoscrypt_chacha_mode3(uint4 *XV) {
    uint4 Y;
    uint i;

    Y = XV[0];

    Y ^= XV[3];

    XV[0] = Y;

    for(i = 0; i < 10; i++)
      chacha_core_mode3((uint *) &Y);

    Y += XV[0];

    XV[0] = Y;

    Y ^= XV[1];

    XV[1] = Y;

    for(i = 0; i < 10; i++)
      chacha_core_mode3((uint *) &Y);

    Y += XV[1];

    XV[1] = Y;

    Y ^= XV[2];

    XV[2] = Y;

    for(i = 0; i < 10; i++)
      chacha_core_mode3((uint *) &Y);

    Y += XV[2];

    XV[2] = Y;

    Y ^= XV[3];

    XV[3] = Y;

    for(i = 0; i < 10; i++)
      chacha_core_mode3((uint *) &Y);

    Y += XV[3];

    XV[3] = Y;

    XV[2] ^= XV[1];
    XV[1] ^= XV[2];
    XV[2] ^= XV[1];
}


__global__ __launch_bounds__(TPB_MIX_MODE1, 1)
void neoscrypt_gpu_hash_salsa_mode1() {
    const uint thrid = blockDim.x * blockIdx.x + threadIdx.x;
    const uint membase = blockDim.x * blockIdx.x * 1024 * 2;
    const uint shiftTr = 8 * thrid;
    uint offset, i;

    uint8 X[8], tmp;

    for(i = 0; i < 8; i++)
    asm("{\n"
      "ld.global.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(X[i].s0), "=r"(X[i].s1), "=r"(X[i].s2), "=r"(X[i].s3),
          "=r"(X[i].s4), "=r"(X[i].s5), "=r"(X[i].s6), "=r"(X[i].s7)
        : __MEM_PTR(&(Input + shiftTr)[i]));

    for(i = 0; i < 128; i++) {
        offset = membase + (i * TPB_MIX_MODE1 + threadIdx.x) * 9;
        ((uint64 *) (G + offset))[0] = ((uint64 *) X)[0];
        neoscrypt_salsa_mode1((uint *) X);
    }

    for(i = 0; i < 128; i++) {
        offset = membase + ((X[6].s0 & 0x7F) * blockDim.x + threadIdx.x) * 9;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[0]));
        X[0] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[1]));
        X[1] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[2]));
        X[2] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[3]));
        X[3] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[4]));
        X[4] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[5]));
        X[5] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[6]));
        X[6] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[7]));
        X[7] ^= tmp;
        neoscrypt_salsa_mode1((uint *) X);
    }

    ((uint64 *) (Tr + shiftTr))[0] = ((uint64 *) X)[0];
}

__global__ __launch_bounds__(TPB_MIX_MODE1, 1)
void neoscrypt_gpu_hash_chacha_mode1() {
    const uint thrid = blockDim.x * blockIdx.x + threadIdx.x;
    const uint membase = blockDim.x * blockIdx.x * 1024 * 2;
    const uint shiftTr = 8 * thrid;
    uint offset, i;

    uint8 X[8], tmp;

    for(i = 0; i < 8; i++)
    asm("{\n"
      "ld.global.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(X[i].s0), "=r"(X[i].s1), "=r"(X[i].s2), "=r"(X[i].s3),
          "=r"(X[i].s4), "=r"(X[i].s5), "=r"(X[i].s6), "=r"(X[i].s7)
        : __MEM_PTR(&(Input + shiftTr)[i]));

    for(i = 0; i < 128; i++) {
        offset = membase + (i * TPB_MIX_MODE1 + threadIdx.x) * 9 + 8;
        ((uint64 *) (G + offset))[0] = ((uint64 *) X)[0];
        neoscrypt_chacha_mode1((uint *) X);
    }

    for(i = 0; i < 128; i++) {
        offset = membase + ((X[6].s0 & 0x7F) * blockDim.x + threadIdx.x) * 9 + 8;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[0]));
        X[0] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[1]));
        X[1] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[2]));
        X[2] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[3]));
        X[3] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[4]));
        X[4] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[5]));
        X[5] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[6]));
        X[6] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[7]));
        X[7] ^= tmp;
        neoscrypt_chacha_mode1((uint *) X);
    }

    ((uint64 *) (Tr2 + shiftTr))[0] = ((uint64 *) X)[0];
}

__global__ __launch_bounds__(TPB_MIX_MODE2, 1)
void neoscrypt_gpu_hash_salsa_mode2() {
    const uint thrid = blockDim.x * blockIdx.x + threadIdx.x;
    const uint membase = thrid * 128 * 8;
    const uint shiftTr = thrid * 8;
    uint offset, i;

    uint8 X[8], tmp;

    for(i = 0; i < 8; i++)
    asm("{\n"
      "ld.global.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(X[i].s0), "=r"(X[i].s1), "=r"(X[i].s2), "=r"(X[i].s3),
          "=r"(X[i].s4), "=r"(X[i].s5), "=r"(X[i].s6), "=r"(X[i].s7)
        : __MEM_PTR(&(Input + shiftTr)[i]));

    for(i = 0; i < 128; i++) {
        offset = membase + i * 8;
        ((uint64 *) (G + offset))[0] = ((uint64 *) X)[0];
        neoscrypt_salsa_mode1((uint *) X);
    }

    for(i = 0; i < 128; i++) {
        offset = membase + (X[6].s0 & 0x7F) * 8;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[0]));
        X[0] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[1]));
        X[1] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[2]));
        X[2] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[3]));
        X[3] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[4]));
        X[4] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[5]));
        X[5] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[6]));
        X[6] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[7]));
        X[7] ^= tmp;
        neoscrypt_salsa_mode1((uint *) X);
    }

    ((uint64 *) (Tr + shiftTr))[0] = ((uint64 *) X)[0];
}

__global__ __launch_bounds__(TPB_MIX_MODE2, 1)
void neoscrypt_gpu_hash_chacha_mode2() {
    const uint thrid = blockDim.x * blockIdx.x + threadIdx.x;
    const uint membase = (gridDim.x * blockDim.x + thrid) * 128 * 8;
    const uint shiftTr = thrid * 8;
    uint offset, i;

    uint8 X[8], tmp;

    for(i = 0; i < 8; i++)
    asm("{\n"
      "ld.global.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(X[i].s0), "=r"(X[i].s1), "=r"(X[i].s2), "=r"(X[i].s3),
          "=r"(X[i].s4), "=r"(X[i].s5), "=r"(X[i].s6), "=r"(X[i].s7)
        : __MEM_PTR(&(Input + shiftTr)[i]));

    for(i = 0; i < 128; i++) {
        offset = membase + i * 8;
        ((uint64 *) (G + offset))[0] = ((uint64 *) X)[0];
        neoscrypt_chacha_mode1((uint *) X);
    }

    for(i = 0; i < 128; i++) {
        offset = membase + (X[6].s0 & 0x7F) * 8;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[0]));
        X[0] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[1]));
        X[1] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[2]));
        X[2] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[3]));
        X[3] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[4]));
        X[4] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[5]));
        X[5] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[6]));
        X[6] ^= tmp;
    asm("{\n"
      "ld.global.nc.v4.u32 {%0, %1, %2, %3}, [%8];\n"
      "ld.global.nc.v4.u32 {%4, %5, %6, %7}, [%8 + 16];\n"
    "}" : "=r"(tmp.s0), "=r"(tmp.s1), "=r"(tmp.s2), "=r"(tmp.s3),
          "=r"(tmp.s4), "=r"(tmp.s5), "=r"(tmp.s6), "=r"(tmp.s7)
        : __MEM_PTR(&(G + offset)[7]));
        X[7] ^= tmp;
        neoscrypt_chacha_mode1((uint *) X);
    }

    ((uint64 *) (Tr2 + shiftTr))[0] = ((uint64 *) X)[0];
}


__global__ __launch_bounds__(TPB_MIX_MODE3, 1)
void neoscrypt_gpu_hash_salsa_mode3() {
    const uint thrid = blockDim.y * blockIdx.x + threadIdx.y;
    const uint shift = 128 * 9 * (thrid & 0x1FFF);
    const uint shiftTr = thrid * 8;
    uint i, j;

    uint __align__(16) X[16];
    
    for(i = 0; i < 4; i++) {
    asm("ld.global.b32 %0, [%1];" : "=r"(X[4 * i])
      : __MEM_PTR((uint *) &(Input + shiftTr)[i * 2] + ((0 + threadIdx.x) & 0x03) * 4 + threadIdx.x));
    asm("ld.global.b32 %0, [%1];" : "=r"(X[4 * i + 1])
      : __MEM_PTR((uint *) &(Input + shiftTr)[i * 2] + ((1 + threadIdx.x) & 0x03) * 4 + threadIdx.x));
    asm("ld.global.b32 %0, [%1];" : "=r"(X[4 * i + 2])
      : __MEM_PTR((uint *) &(Input + shiftTr)[i * 2] + ((2 + threadIdx.x) & 0x03) * 4 + threadIdx.x));
    asm("ld.global.b32 %0, [%1];" : "=r"(X[4 * i + 3])
      : __MEM_PTR((uint *) &(Input + shiftTr)[i * 2] + ((3 + threadIdx.x) & 0x03) * 4 + threadIdx.x));
    }

    for(i = 0; i < 128; i++) {
        uint offset = shift + i * 8;
        for(j = 0; j < 4; j++)
          ((uint4 *) (G + offset))[j * 4 + threadIdx.x] = ((uint4 *) X)[j];
        neoscrypt_salsa_mode3((uint4 *) X);
    }

    for(i = 0; i < 128; i++) {
        uint offset;
    asm("shfl.idx.b32 %0, %1, 0, 0x1C1F;" : "=r"(offset) : "r"(X[12]));
        offset = shift + (offset & 0x7F) * 8;
        for(j = 0; j < 4; j++)
          ((uint4 *) X)[j] ^= ((uint4 *) (G + offset))[j * 4 + threadIdx.x];
        neoscrypt_salsa_mode3((uint4 *) X);
    }

    for(i = 0; i < 4; i++) {
    asm("st.global.b32 [%1], %0;" : : "r"(X[4 * i]),
      __MEM_PTR((uint *) &(Tr2 + shiftTr)[2 * i] + ((0 + threadIdx.x) & 0x03) * 4 + threadIdx.x));
    asm("st.global.b32 [%1], %0;" : : "r"(X[4 * i + 1]),
      __MEM_PTR((uint *) &(Tr2 + shiftTr)[2 * i] + ((1 + threadIdx.x) & 0x03) * 4 + threadIdx.x));
    asm("st.global.b32 [%1], %0;" : : "r"(X[4 * i + 2]),
      __MEM_PTR((uint *) &(Tr2 + shiftTr)[2 * i] + ((2 + threadIdx.x) & 0x03) * 4 + threadIdx.x));
    asm("st.global.b32 [%1], %0;" : : "r"(X[4 * i + 3]),
      __MEM_PTR((uint *) &(Tr2 + shiftTr)[2 * i] + ((3 + threadIdx.x) & 0x03) * 4 + threadIdx.x));
    }
}

__global__ __launch_bounds__(TPB_MIX_MODE3, 1)
void neoscrypt_gpu_hash_chacha_mode3() {
    const uint thrid = blockDim.y * blockIdx.x + threadIdx.y;
    const uint shift = 128 * 9 * (thrid & 0x1FFF) + 128 * 8;
    const uint shiftTr = thrid * 8;
    uint i, j;

    uint __align__(16) X[16];

    for(i = 0; i < 4; i++) {
    asm("ld.global.b32 %0, [%1];" : "=r"(X[4 * i])
      : __MEM_PTR((uint *) &(Input + shiftTr)[i * 2] + 0 * 4 + threadIdx.x));
    asm("ld.global.b32 %0, [%1];" : "=r"(X[4 * i + 1])
      : __MEM_PTR((uint *) &(Input + shiftTr)[i * 2] + 1 * 4 + threadIdx.x));
    asm("ld.global.b32 %0, [%1];" : "=r"(X[4 * i + 2])
      : __MEM_PTR((uint *) &(Input + shiftTr)[i * 2] + 2 * 4 + threadIdx.x));
    asm("ld.global.b32 %0, [%1];" : "=r"(X[4 * i + 3])
      : __MEM_PTR((uint *) &(Input + shiftTr)[i * 2] + 3 * 4 + threadIdx.x));
    }

    for(i = 0; i < 128; i++) {
        uint offset = shift + i * 8;
        for(j = 0; j < 4; j++)
          ((uint4 *) (G + offset))[j * 4 + threadIdx.x] = ((uint4 *) X)[j];
        neoscrypt_chacha_mode3((uint4 *) X);
    }

    for(i = 0; i < 128; i++) {
        uint offset;
    asm("shfl.idx.b32 %0, %1, 0, 7199;" : "=r"(offset) : "r"(X[12]));
        offset = shift + (offset & 0x7F) * 8;
        for(j = 0; j < 4; j++)
          ((uint4 *) X)[j] ^= ((uint4 *) (G + offset))[j * 4 + threadIdx.x];
        neoscrypt_chacha_mode3((uint4 *) X);
    }

    for(i = 0; i < 4; i++) {
    asm("st.global.b32 [%1], %0;" : : "r"(X[4 * i]),
      __MEM_PTR((uint *) &(Tr + shiftTr)[2 * i] + 0 * 4 + threadIdx.x));
    asm("st.global.b32 [%1], %0;" : : "r"(X[4 * i + 1]),
      __MEM_PTR((uint *) &(Tr + shiftTr)[2 * i] + 1 * 4 + threadIdx.x));
    asm("st.global.b32 [%1], %0;" : : "r"(X[4 * i + 2]),
      __MEM_PTR((uint *) &(Tr + shiftTr)[2 * i] + 2 * 4 + threadIdx.x));
    asm("st.global.b32 [%1], %0;" : : "r"(X[4 * i + 3]),
      __MEM_PTR((uint *) &(Tr + shiftTr)[2 * i] + 3 * 4 + threadIdx.x));
    }
}


__host__ uint neoscrypt_hash(uint thr_id, uint throughput, uint startNonce,
  uint hash_mode) {
    uint result[MAX_GPUS] = { 0xFFFFFFFF };

    cudaMemset(Nonce[thr_id], 0xFF, 4);

    dim3 grid(throughput / TPB, 1, 1);
    dim3 block(TPB, 1, 1);

    dim3 grid_mix1(throughput / TPB_MIX_MODE1, 1, 1);
    dim3 block_mix1(TPB_MIX_MODE1, 1, 1);

    dim3 grid_mix2(throughput / TPB_MIX_MODE2, 1, 1);
    dim3 block_mix2(TPB_MIX_MODE2, 1, 1);

    dim3 grid_mix3((throughput * 4) / TPB_MIX_MODE3);
    dim3 block_mix3(4, TPB_MIX_MODE3 / 4);

    cudaStream_t stream[2];
    cudaStreamCreate(&stream[0]);
    cudaStreamCreate(&stream[1]);

    neoscrypt_gpu_hash_start <<<grid, block>>> (startNonce);

    switch(hash_mode) {

        default:
        case(1):
            neoscrypt_gpu_hash_salsa_mode1 <<<grid_mix1, block_mix1, 0, stream[0]>>> ();
            neoscrypt_gpu_hash_chacha_mode1 <<<grid_mix1, block_mix1, 0, stream[1]>>> ();
            break;

        case(2):
            neoscrypt_gpu_hash_salsa_mode2 <<<grid_mix2, block_mix2, 0, stream[0]>>> ();
            neoscrypt_gpu_hash_chacha_mode2 <<<grid_mix2, block_mix2, 0, stream[1]>>> ();
            break;

        case(3):
            neoscrypt_gpu_hash_salsa_mode3 <<<grid_mix3, block_mix3, 0, stream[0]>>> ();
            neoscrypt_gpu_hash_chacha_mode3 <<<grid_mix3, block_mix3, 0, stream[1]>>> ();
            break;

    }

    cudaDeviceSynchronize();

    neoscrypt_gpu_hash_end <<<grid, block>>> (startNonce, Nonce[thr_id]);

    cudaMemcpy(&result[thr_id], Nonce[thr_id], sizeof(uint), cudaMemcpyDeviceToHost);

    cudaStreamDestroy(stream[0]);
    cudaStreamDestroy(stream[1]);

    return(result[thr_id]);
}

__host__ void neoscrypt_init(uint thr_id, uint *gmem, uint *hash0, uint *hash1, uint *hash2) {
    cudaMemcpyToSymbolAsync(G, &gmem, sizeof(gmem), 0, cudaMemcpyHostToDevice);
    cudaMemcpyToSymbolAsync(Tr, &hash0, sizeof(hash0), 0, cudaMemcpyHostToDevice);
    cudaMemcpyToSymbolAsync(Tr2, &hash1, sizeof(hash1), 0, cudaMemcpyHostToDevice);
    cudaMemcpyToSymbolAsync(Input, &hash2, sizeof(hash2), 0, cudaMemcpyHostToDevice);
    cudaMalloc(&Nonce[thr_id], 4);
}

__host__ void neoscrypt_prehash(uint *pdata, const uint *ptarget) {
    uint PaddedMessage[64], input[16], key[16] = {0}, i;

    for(i = 0; i < 19; i++) {
        PaddedMessage[i] = pdata[i];
        PaddedMessage[i + 20] = pdata[i];
        PaddedMessage[i + 40] = pdata[i];
    }

    for(i = 0; i < 4; i++)
      PaddedMessage[i + 60] = pdata[i];

    PaddedMessage[19] = 0;
    PaddedMessage[39] = 0;
    PaddedMessage[59] = 0;

    ((uint8 *) input)[0] = ((uint8 *) pdata)[0];
    ((uint8 *) input)[1] = ((uint8 *) pdata)[1];
    ((uint8 *) key)[0] = ((uint8 *) pdata)[0];

    blake2s_host(input, key);

    cudaMemcpyToSymbolAsync(c_data, PaddedMessage, 256, 0, cudaMemcpyHostToDevice);
    cudaMemcpyToSymbolAsync(input_init, input, 64, 0, cudaMemcpyHostToDevice);
    cudaMemcpyToSymbolAsync(key_init, key, 64, 0, cudaMemcpyHostToDevice);
    cudaMemcpyToSymbolAsync(hash_target, &ptarget[7], 4, 0, cudaMemcpyHostToDevice);

    cudaGetLastError();
}
